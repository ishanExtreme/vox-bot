Nov 19 23:33:56 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 19 23:33:57 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 19 23:33:57 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 19 23:33:57 ishan-Legion systemd[1]: ollama.service: Consumed 7.405s CPU time.
-- Boot a0e6ee6ef2ea47bf9eefc93e20349c47 --
Nov 20 00:06:29 ishan-Legion systemd[1]: Started Ollama Service.
Nov 20 00:06:29 ishan-Legion ollama[2735]: 2024/11/20 00:06:29 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 20 00:06:29 ishan-Legion ollama[2735]: time=2024-11-20T00:06:29.993+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 20 00:06:29 ishan-Legion ollama[2735]: time=2024-11-20T00:06:29.993+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 20 00:06:29 ishan-Legion ollama[2735]: time=2024-11-20T00:06:29.994+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 20 00:06:29 ishan-Legion ollama[2735]: time=2024-11-20T00:06:29.994+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama4226788151/runners
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.472+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.472+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.561+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.562+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.562+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 20 00:06:32 ishan-Legion ollama[2735]: time=2024-11-20T00:06:32.562+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 20 02:22:16 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 20 02:22:16 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 20 02:22:16 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 20 02:22:16 ishan-Legion systemd[1]: ollama.service: Consumed 7.471s CPU time.
-- Boot 428f2a2150cf42a286d7b77bbc6d437d --
Nov 20 16:00:09 ishan-Legion systemd[1]: Started Ollama Service.
Nov 20 16:00:09 ishan-Legion ollama[2624]: 2024/11/20 16:00:09 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 20 16:00:09 ishan-Legion ollama[2624]: time=2024-11-20T16:00:09.917+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 20 16:00:09 ishan-Legion ollama[2624]: time=2024-11-20T16:00:09.917+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 20 16:00:09 ishan-Legion ollama[2624]: time=2024-11-20T16:00:09.918+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 20 16:00:09 ishan-Legion ollama[2624]: time=2024-11-20T16:00:09.919+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1755632122/runners
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.311+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.311+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.401+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.401+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.401+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 20 16:00:12 ishan-Legion ollama[2624]: time=2024-11-20T16:00:12.402+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 20 17:46:53 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 20 17:46:54 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 20 17:46:54 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 20 17:46:54 ishan-Legion systemd[1]: ollama.service: Consumed 7.161s CPU time.
-- Boot 7a2ed45f9eee4b419078824ed17d19e2 --
Nov 20 22:14:33 ishan-Legion systemd[1]: Started Ollama Service.
Nov 20 22:14:33 ishan-Legion ollama[2456]: 2024/11/20 22:14:33 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 20 22:14:33 ishan-Legion ollama[2456]: time=2024-11-20T22:14:33.966+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 20 22:14:33 ishan-Legion ollama[2456]: time=2024-11-20T22:14:33.966+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 20 22:14:33 ishan-Legion ollama[2456]: time=2024-11-20T22:14:33.967+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 20 22:14:33 ishan-Legion ollama[2456]: time=2024-11-20T22:14:33.967+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1163730915/runners
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.507+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.508+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.583+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.583+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.583+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 20 22:14:36 ishan-Legion ollama[2456]: time=2024-11-20T22:14:36.583+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 21 00:35:06 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 21 00:35:06 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 21 00:35:06 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 21 00:35:06 ishan-Legion systemd[1]: ollama.service: Consumed 7.644s CPU time.
-- Boot 23a01dc83e0e4fc5b9966c2a4589c5a8 --
Nov 21 18:57:23 ishan-Legion systemd[1]: Started Ollama Service.
Nov 21 18:57:23 ishan-Legion ollama[2600]: 2024/11/21 18:57:23 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 21 18:57:23 ishan-Legion ollama[2600]: time=2024-11-21T18:57:23.400+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 21 18:57:23 ishan-Legion ollama[2600]: time=2024-11-21T18:57:23.400+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 21 18:57:23 ishan-Legion ollama[2600]: time=2024-11-21T18:57:23.401+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 21 18:57:23 ishan-Legion ollama[2600]: time=2024-11-21T18:57:23.402+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama644627241/runners
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.842+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.842+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.942+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.945+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.945+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 21 18:57:25 ishan-Legion ollama[2600]: time=2024-11-21T18:57:25.945+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 21 22:32:06 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 21 22:32:07 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 21 22:32:07 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 21 22:32:07 ishan-Legion systemd[1]: ollama.service: Consumed 7.449s CPU time.
-- Boot 7933b90c8fbc4d17ab9834ebd2bf7930 --
Nov 22 00:06:31 ishan-Legion systemd[1]: Started Ollama Service.
Nov 22 00:06:31 ishan-Legion ollama[2567]: 2024/11/22 00:06:31 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 22 00:06:31 ishan-Legion ollama[2567]: time=2024-11-22T00:06:31.699+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 22 00:06:31 ishan-Legion ollama[2567]: time=2024-11-22T00:06:31.699+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 22 00:06:31 ishan-Legion ollama[2567]: time=2024-11-22T00:06:31.699+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 22 00:06:31 ishan-Legion ollama[2567]: time=2024-11-22T00:06:31.700+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama54183411/runners
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.171+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.171+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.246+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.247+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.247+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 22 00:06:34 ishan-Legion ollama[2567]: time=2024-11-22T00:06:34.247+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 22 00:47:22 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 22 00:47:22 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 22 00:47:22 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 22 00:47:22 ishan-Legion systemd[1]: ollama.service: Consumed 7.493s CPU time.
-- Boot 4c30d189091744058ca906ae4a778c66 --
Nov 22 19:59:53 ishan-Legion systemd[1]: Started Ollama Service.
Nov 22 19:59:53 ishan-Legion ollama[2507]: 2024/11/22 19:59:53 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 22 19:59:53 ishan-Legion ollama[2507]: time=2024-11-22T19:59:53.443+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 22 19:59:53 ishan-Legion ollama[2507]: time=2024-11-22T19:59:53.443+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 22 19:59:53 ishan-Legion ollama[2507]: time=2024-11-22T19:59:53.443+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 22 19:59:53 ishan-Legion ollama[2507]: time=2024-11-22T19:59:53.444+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3995749171/runners
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.876+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.877+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.951+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.952+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.952+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 22 19:59:55 ishan-Legion ollama[2507]: time=2024-11-22T19:59:55.952+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 22 20:09:40 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 22 20:09:40 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 22 20:09:40 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 22 20:09:40 ishan-Legion systemd[1]: ollama.service: Consumed 7.373s CPU time.
-- Boot 60010a47ec6344419e890dbec8c1e6c9 --
Nov 23 05:15:13 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 05:15:13 ishan-Legion ollama[2588]: 2024/11/23 05:15:13 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 05:15:13 ishan-Legion ollama[2588]: time=2024-11-23T05:15:13.440+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 05:15:13 ishan-Legion ollama[2588]: time=2024-11-23T05:15:13.440+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 05:15:13 ishan-Legion ollama[2588]: time=2024-11-23T05:15:13.441+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 05:15:13 ishan-Legion ollama[2588]: time=2024-11-23T05:15:13.441+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3825518148/runners
Nov 23 05:15:15 ishan-Legion ollama[2588]: time=2024-11-23T05:15:15.927+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 23 05:15:15 ishan-Legion ollama[2588]: time=2024-11-23T05:15:15.927+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 05:15:16 ishan-Legion ollama[2588]: time=2024-11-23T05:15:16.016+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 05:15:16 ishan-Legion ollama[2588]: time=2024-11-23T05:15:16.017+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 05:15:16 ishan-Legion ollama[2588]: time=2024-11-23T05:15:16.017+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 05:15:16 ishan-Legion ollama[2588]: time=2024-11-23T05:15:16.017+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 23 11:35:32 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 11:35:33 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 11:35:33 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 11:35:33 ishan-Legion systemd[1]: ollama.service: Consumed 7.833s CPU time.
-- Boot 14181355b5db48db8549c9b83bb482c8 --
Nov 23 13:21:58 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 13:21:58 ishan-Legion ollama[2561]: 2024/11/23 13:21:58 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 13:21:58 ishan-Legion ollama[2561]: time=2024-11-23T13:21:58.371+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 13:21:58 ishan-Legion ollama[2561]: time=2024-11-23T13:21:58.372+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 13:21:58 ishan-Legion ollama[2561]: time=2024-11-23T13:21:58.372+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 13:21:58 ishan-Legion ollama[2561]: time=2024-11-23T13:21:58.373+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama4273731806/runners
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.735+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.736+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.811+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.812+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.812+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 13:22:00 ishan-Legion ollama[2561]: time=2024-11-23T13:22:00.812+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 23 20:54:09 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 20:54:09 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 20:54:09 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 20:54:09 ishan-Legion systemd[1]: ollama.service: Consumed 7.543s CPU time.
-- Boot 6e437c47a41840ad9bf00c77f832d185 --
Nov 23 21:25:54 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 21:25:54 ishan-Legion ollama[2570]: 2024/11/23 21:25:54 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 21:25:54 ishan-Legion ollama[2570]: time=2024-11-23T21:25:54.952+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 21:25:54 ishan-Legion ollama[2570]: time=2024-11-23T21:25:54.952+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 21:25:54 ishan-Legion ollama[2570]: time=2024-11-23T21:25:54.953+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 21:25:54 ishan-Legion ollama[2570]: time=2024-11-23T21:25:54.954+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3618015982/runners
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.340+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.340+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.431+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.432+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.432+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 21:25:57 ishan-Legion ollama[2570]: time=2024-11-23T21:25:57.432+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 23 21:28:21 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 21:28:21 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 21:28:21 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 21:28:21 ishan-Legion systemd[1]: ollama.service: Consumed 7.180s CPU time.
-- Boot 01662b417c414d439a689e97c3420a91 --
Nov 23 21:28:49 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 21:28:49 ishan-Legion ollama[2308]: 2024/11/23 21:28:49 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 21:28:49 ishan-Legion ollama[2308]: time=2024-11-23T21:28:49.401+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 21:28:49 ishan-Legion ollama[2308]: time=2024-11-23T21:28:49.402+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 21:28:49 ishan-Legion ollama[2308]: time=2024-11-23T21:28:49.402+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 21:28:49 ishan-Legion ollama[2308]: time=2024-11-23T21:28:49.403+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1172303014/runners
Nov 23 21:28:51 ishan-Legion ollama[2308]: time=2024-11-23T21:28:51.867+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 23 21:28:51 ishan-Legion ollama[2308]: time=2024-11-23T21:28:51.867+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 21:28:52 ishan-Legion ollama[2308]: time=2024-11-23T21:28:52.244+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 21:28:52 ishan-Legion ollama[2308]: time=2024-11-23T21:28:52.244+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 21:28:52 ishan-Legion ollama[2308]: time=2024-11-23T21:28:52.244+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 21:28:52 ishan-Legion ollama[2308]: time=2024-11-23T21:28:52.244+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 23 21:30:07 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 21:30:07 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 21:30:07 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 21:30:07 ishan-Legion systemd[1]: ollama.service: Consumed 7.686s CPU time.
-- Boot 44b84725718242d9b7472a282e8830b8 --
Nov 23 21:30:46 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 21:30:46 ishan-Legion ollama[2384]: 2024/11/23 21:30:46 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 21:30:46 ishan-Legion ollama[2384]: time=2024-11-23T21:30:46.427+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 21:30:46 ishan-Legion ollama[2384]: time=2024-11-23T21:30:46.427+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 21:30:46 ishan-Legion ollama[2384]: time=2024-11-23T21:30:46.428+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 21:30:46 ishan-Legion ollama[2384]: time=2024-11-23T21:30:46.428+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1536555308/runners
Nov 23 21:30:48 ishan-Legion ollama[2384]: time=2024-11-23T21:30:48.951+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]"
Nov 23 21:30:48 ishan-Legion ollama[2384]: time=2024-11-23T21:30:48.951+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 21:30:49 ishan-Legion ollama[2384]: time=2024-11-23T21:30:49.027+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 21:30:49 ishan-Legion ollama[2384]: time=2024-11-23T21:30:49.027+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 21:30:49 ishan-Legion ollama[2384]: time=2024-11-23T21:30:49.027+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 21:30:49 ishan-Legion ollama[2384]: time=2024-11-23T21:30:49.027+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 23 21:36:14 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 21:36:14 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 21:36:14 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 21:36:14 ishan-Legion systemd[1]: ollama.service: Consumed 7.661s CPU time.
-- Boot 2b2947f565f84350b175acf57949a469 --
Nov 23 21:37:02 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 21:37:02 ishan-Legion ollama[2669]: 2024/11/23 21:37:02 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 21:37:02 ishan-Legion ollama[2669]: time=2024-11-23T21:37:02.925+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 21:37:02 ishan-Legion ollama[2669]: time=2024-11-23T21:37:02.926+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 21:37:02 ishan-Legion ollama[2669]: time=2024-11-23T21:37:02.926+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 21:37:02 ishan-Legion ollama[2669]: time=2024-11-23T21:37:02.927+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama411480768/runners
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.431+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.431+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.527+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.528+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.528+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 21:37:05 ishan-Legion ollama[2669]: time=2024-11-23T21:37:05.528+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Nov 23 22:15:54 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 23 22:15:54 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 23 22:15:54 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 23 22:15:54 ishan-Legion systemd[1]: ollama.service: Consumed 7.354s CPU time.
-- Boot dcb60b10e21740d4bb3e50deb9eff631 --
Nov 23 22:17:56 ishan-Legion systemd[1]: Started Ollama Service.
Nov 23 22:17:56 ishan-Legion ollama[2649]: 2024/11/23 22:17:56 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 23 22:17:56 ishan-Legion ollama[2649]: time=2024-11-23T22:17:56.083+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 23 22:17:56 ishan-Legion ollama[2649]: time=2024-11-23T22:17:56.084+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 23 22:17:56 ishan-Legion ollama[2649]: time=2024-11-23T22:17:56.084+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 23 22:17:56 ishan-Legion ollama[2649]: time=2024-11-23T22:17:56.085+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama258130507/runners
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.617+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]"
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.617+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.694+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.695+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.695+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 23 22:17:58 ishan-Legion ollama[2649]: time=2024-11-23T22:17:58.695+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 24 21:55:19 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 24 21:55:19 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 24 21:55:19 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 24 21:55:19 ishan-Legion systemd[1]: ollama.service: Consumed 8.918s CPU time.
-- Boot d0cb620b84554b8c8a69a6240a0f8570 --
Nov 24 21:56:14 ishan-Legion systemd[1]: Started Ollama Service.
Nov 24 21:56:14 ishan-Legion ollama[2400]: 2024/11/24 21:56:14 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Nov 24 21:56:14 ishan-Legion ollama[2400]: time=2024-11-24T21:56:14.432+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Nov 24 21:56:14 ishan-Legion ollama[2400]: time=2024-11-24T21:56:14.433+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Nov 24 21:56:14 ishan-Legion ollama[2400]: time=2024-11-24T21:56:14.433+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Nov 24 21:56:14 ishan-Legion ollama[2400]: time=2024-11-24T21:56:14.434+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3761030803/runners
Nov 24 21:56:16 ishan-Legion ollama[2400]: time=2024-11-24T21:56:16.974+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Nov 24 21:56:16 ishan-Legion ollama[2400]: time=2024-11-24T21:56:16.974+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Nov 24 21:56:17 ishan-Legion ollama[2400]: time=2024-11-24T21:56:17.050+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Nov 24 21:56:17 ishan-Legion ollama[2400]: time=2024-11-24T21:56:17.051+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Nov 24 21:56:17 ishan-Legion ollama[2400]: time=2024-11-24T21:56:17.051+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Nov 24 21:56:17 ishan-Legion ollama[2400]: time=2024-11-24T21:56:17.051+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Nov 25 02:55:20 ishan-Legion systemd[1]: Stopping Ollama Service...
Nov 25 02:55:20 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Nov 25 02:55:20 ishan-Legion systemd[1]: Stopped Ollama Service.
Nov 25 02:55:20 ishan-Legion systemd[1]: ollama.service: Consumed 7.867s CPU time.
-- Boot 0e9ee86c08ea48a09f53001b261864ba --
Dec 05 01:02:24 ishan-Legion systemd[1]: Started Ollama Service.
Dec 05 01:02:24 ishan-Legion ollama[2319]: 2024/12/05 01:02:24 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 05 01:02:24 ishan-Legion ollama[2319]: time=2024-12-05T01:02:24.127+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 05 01:02:24 ishan-Legion ollama[2319]: time=2024-12-05T01:02:24.127+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 05 01:02:24 ishan-Legion ollama[2319]: time=2024-12-05T01:02:24.127+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 05 01:02:24 ishan-Legion ollama[2319]: time=2024-12-05T01:02:24.128+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1696875836/runners
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.744+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.744+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.934+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.934+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.934+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 05 01:02:26 ishan-Legion ollama[2319]: time=2024-12-05T01:02:26.934+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Dec 05 16:55:57 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 05 16:55:57 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 05 16:55:57 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 05 16:55:57 ishan-Legion systemd[1]: ollama.service: Consumed 7.994s CPU time.
-- Boot a1d0c9fb46194052a39c649f9e23ed43 --
Dec 08 13:36:27 ishan-Legion systemd[1]: Started Ollama Service.
Dec 08 13:36:27 ishan-Legion ollama[2532]: 2024/12/08 13:36:27 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 08 13:36:27 ishan-Legion ollama[2532]: time=2024-12-08T13:36:27.426+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 08 13:36:27 ishan-Legion ollama[2532]: time=2024-12-08T13:36:27.426+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 08 13:36:27 ishan-Legion ollama[2532]: time=2024-12-08T13:36:27.427+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 08 13:36:27 ishan-Legion ollama[2532]: time=2024-12-08T13:36:27.427+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3985079974/runners
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.880+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.880+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.969+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.969+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.969+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 08 13:36:29 ishan-Legion ollama[2532]: time=2024-12-08T13:36:29.969+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Dec 08 13:55:22 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 08 13:55:24 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 08 13:55:24 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 08 13:55:24 ishan-Legion systemd[1]: ollama.service: Consumed 7.309s CPU time.
-- Boot 76c76960b5f8477fbc58faf7b896da6f --
Dec 12 21:03:02 ishan-Legion systemd[1]: Started Ollama Service.
Dec 12 21:03:02 ishan-Legion ollama[2562]: 2024/12/12 21:03:02 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 12 21:03:02 ishan-Legion ollama[2562]: time=2024-12-12T21:03:02.224+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 12 21:03:02 ishan-Legion ollama[2562]: time=2024-12-12T21:03:02.224+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 12 21:03:02 ishan-Legion ollama[2562]: time=2024-12-12T21:03:02.225+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 12 21:03:02 ishan-Legion ollama[2562]: time=2024-12-12T21:03:02.225+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama606511434/runners
-- Boot c33e9acd9bc34fec954786516a858553 --
Dec 15 20:37:12 ishan-Legion systemd[1]: Started Ollama Service.
Dec 15 20:37:12 ishan-Legion ollama[4050]: 2024/12/15 20:37:12 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 15 20:37:12 ishan-Legion ollama[4050]: time=2024-12-15T20:37:12.704+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 15 20:37:12 ishan-Legion ollama[4050]: time=2024-12-15T20:37:12.705+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 15 20:37:12 ishan-Legion ollama[4050]: time=2024-12-15T20:37:12.705+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 15 20:37:12 ishan-Legion ollama[4050]: time=2024-12-15T20:37:12.706+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2126752885/runners
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.382+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.382+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.500+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.501+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.501+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 15 20:37:15 ishan-Legion ollama[4050]: time=2024-12-15T20:37:15.501+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.2 GiB"
Dec 15 20:38:43 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 15 20:38:44 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 15 20:38:44 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 15 20:38:44 ishan-Legion systemd[1]: ollama.service: Consumed 8.140s CPU time.
-- Boot fb5136b460db4ee3a885489e0277dfde --
Dec 26 03:20:08 ishan-Legion systemd[1]: Started Ollama Service.
Dec 26 03:20:08 ishan-Legion ollama[2620]: 2024/12/26 03:20:08 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 26 03:20:08 ishan-Legion ollama[2620]: time=2024-12-26T03:20:08.673+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 26 03:20:08 ishan-Legion ollama[2620]: time=2024-12-26T03:20:08.674+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 26 03:20:08 ishan-Legion ollama[2620]: time=2024-12-26T03:20:08.674+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 26 03:20:08 ishan-Legion ollama[2620]: time=2024-12-26T03:20:08.675+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1876427788/runners
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.096+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.096+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.170+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.170+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.170+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 26 03:20:11 ishan-Legion ollama[2620]: time=2024-12-26T03:20:11.170+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Dec 26 03:51:28 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 26 03:51:28 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 26 03:51:28 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 26 03:51:28 ishan-Legion systemd[1]: ollama.service: Consumed 7.272s CPU time.
-- Boot 99bf12ee67664424bc18c0053e5c1852 --
Dec 26 12:16:08 ishan-Legion systemd[1]: Started Ollama Service.
Dec 26 12:16:08 ishan-Legion ollama[2507]: 2024/12/26 12:16:08 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 26 12:16:08 ishan-Legion ollama[2507]: time=2024-12-26T12:16:08.865+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 26 12:16:08 ishan-Legion ollama[2507]: time=2024-12-26T12:16:08.866+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 26 12:16:08 ishan-Legion ollama[2507]: time=2024-12-26T12:16:08.866+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 26 12:16:08 ishan-Legion ollama[2507]: time=2024-12-26T12:16:08.867+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3771133282/runners
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.293+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.293+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.381+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.382+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.382+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 26 12:16:11 ishan-Legion ollama[2507]: time=2024-12-26T12:16:11.382+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Dec 26 13:07:54 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 26 13:07:54 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 26 13:07:54 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 26 13:07:54 ishan-Legion systemd[1]: ollama.service: Consumed 7.332s CPU time.
-- Boot efc981c7ccba4a22b3739dc220271b5f --
Dec 26 18:44:00 ishan-Legion systemd[1]: Started Ollama Service.
Dec 26 18:44:00 ishan-Legion ollama[2599]: 2024/12/26 18:44:00 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 26 18:44:00 ishan-Legion ollama[2599]: time=2024-12-26T18:44:00.644+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 26 18:44:00 ishan-Legion ollama[2599]: time=2024-12-26T18:44:00.645+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 26 18:44:00 ishan-Legion ollama[2599]: time=2024-12-26T18:44:00.645+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 26 18:44:00 ishan-Legion ollama[2599]: time=2024-12-26T18:44:00.646+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2724941685/runners
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.132+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]"
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.132+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.213+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.213+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.213+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 26 18:44:03 ishan-Legion ollama[2599]: time=2024-12-26T18:44:03.213+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Dec 26 19:22:05 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 26 19:22:06 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 26 19:22:06 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 26 19:22:06 ishan-Legion systemd[1]: ollama.service: Consumed 7.863s CPU time.
-- Boot 01853d875168419e90cb8a66c37a61de --
Dec 27 09:32:25 ishan-Legion systemd[1]: Started Ollama Service.
Dec 27 09:32:25 ishan-Legion ollama[2689]: 2024/12/27 09:32:25 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 27 09:32:25 ishan-Legion ollama[2689]: time=2024-12-27T09:32:25.822+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 27 09:32:25 ishan-Legion ollama[2689]: time=2024-12-27T09:32:25.823+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 27 09:32:25 ishan-Legion ollama[2689]: time=2024-12-27T09:32:25.824+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 27 09:32:25 ishan-Legion ollama[2689]: time=2024-12-27T09:32:25.825+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2261511009/runners
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.764+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.765+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.871+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.872+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.872+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 27 09:32:30 ishan-Legion ollama[2689]: time=2024-12-27T09:32:30.872+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Dec 27 09:32:32 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 27 09:32:33 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 27 09:32:33 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 27 09:32:33 ishan-Legion systemd[1]: ollama.service: Consumed 14.990s CPU time.
-- Boot 9fa8ff9a90db4913a10445c5c5557bdd --
Dec 28 21:30:02 ishan-Legion systemd[1]: Started Ollama Service.
Dec 28 21:30:02 ishan-Legion ollama[2597]: 2024/12/28 21:30:02 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Dec 28 21:30:02 ishan-Legion ollama[2597]: time=2024-12-28T21:30:02.418+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Dec 28 21:30:02 ishan-Legion ollama[2597]: time=2024-12-28T21:30:02.419+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Dec 28 21:30:02 ishan-Legion ollama[2597]: time=2024-12-28T21:30:02.419+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Dec 28 21:30:02 ishan-Legion ollama[2597]: time=2024-12-28T21:30:02.420+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3627903141/runners
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.166+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.167+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.322+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.323+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.323+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Dec 28 21:30:08 ishan-Legion ollama[2597]: time=2024-12-28T21:30:08.323+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Dec 28 21:30:10 ishan-Legion systemd[1]: Stopping Ollama Service...
Dec 28 21:30:10 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Dec 28 21:30:10 ishan-Legion systemd[1]: Stopped Ollama Service.
Dec 28 21:30:10 ishan-Legion systemd[1]: ollama.service: Consumed 15.940s CPU time.
-- Boot e50f5cb058784330b06aedfe7aa0eabf --
Jan 07 20:16:47 ishan-Legion systemd[1]: Started Ollama Service.
Jan 07 20:16:47 ishan-Legion ollama[2536]: 2025/01/07 20:16:47 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 07 20:16:47 ishan-Legion ollama[2536]: time=2025-01-07T20:16:47.423+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 07 20:16:47 ishan-Legion ollama[2536]: time=2025-01-07T20:16:47.424+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 07 20:16:47 ishan-Legion ollama[2536]: time=2025-01-07T20:16:47.424+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 07 20:16:47 ishan-Legion ollama[2536]: time=2025-01-07T20:16:47.425+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1105208752/runners
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.763+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.763+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.879+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.880+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.880+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 07 20:16:53 ishan-Legion ollama[2536]: time=2025-01-07T20:16:53.880+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Jan 07 20:18:05 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 07 20:18:06 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 07 20:18:06 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 07 20:18:06 ishan-Legion systemd[1]: ollama.service: Consumed 21.085s CPU time.
-- Boot b22cdd27f2874b4c89fe7fbb4e32aa4d --
Jan 15 21:22:48 ishan-Legion systemd[1]: Started Ollama Service.
Jan 15 21:22:48 ishan-Legion ollama[2496]: 2025/01/15 21:22:48 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 15 21:22:48 ishan-Legion ollama[2496]: time=2025-01-15T21:22:48.933+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 15 21:22:48 ishan-Legion ollama[2496]: time=2025-01-15T21:22:48.934+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 15 21:22:48 ishan-Legion ollama[2496]: time=2025-01-15T21:22:48.934+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 15 21:22:48 ishan-Legion ollama[2496]: time=2025-01-15T21:22:48.935+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama368336559/runners
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.229+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.230+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.316+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.317+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.317+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 15 21:22:54 ishan-Legion ollama[2496]: time=2025-01-15T21:22:54.317+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Jan 15 21:24:42 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 15 21:24:43 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 15 21:24:43 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 15 21:24:43 ishan-Legion systemd[1]: ollama.service: Consumed 18.462s CPU time.
-- Boot 9d5cb19e5cdb44dabc078bbac49bff50 --
Jan 19 21:56:27 ishan-Legion systemd[1]: Started Ollama Service.
Jan 19 21:56:28 ishan-Legion ollama[2515]: 2025/01/19 21:56:28 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 19 21:56:28 ishan-Legion ollama[2515]: time=2025-01-19T21:56:28.015+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 19 21:56:28 ishan-Legion ollama[2515]: time=2025-01-19T21:56:28.015+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 19 21:56:28 ishan-Legion ollama[2515]: time=2025-01-19T21:56:28.016+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 19 21:56:28 ishan-Legion ollama[2515]: time=2025-01-19T21:56:28.017+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2563706313/runners
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.262+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.262+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.412+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.413+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.414+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 19 21:56:33 ishan-Legion ollama[2515]: time=2025-01-19T21:56:33.414+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.4 GiB"
Jan 19 23:15:37 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 19 23:15:38 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 19 23:15:38 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 19 23:15:38 ishan-Legion systemd[1]: ollama.service: Consumed 15.326s CPU time.
-- Boot 072eb384af744faf8e4e96dd9fef87ad --
Jan 23 15:19:17 ishan-Legion systemd[1]: Started Ollama Service.
Jan 23 15:19:17 ishan-Legion ollama[2557]: 2025/01/23 15:19:17 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 23 15:19:17 ishan-Legion ollama[2557]: time=2025-01-23T15:19:17.804+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 23 15:19:17 ishan-Legion ollama[2557]: time=2025-01-23T15:19:17.805+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 23 15:19:17 ishan-Legion ollama[2557]: time=2025-01-23T15:19:17.805+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 23 15:19:17 ishan-Legion ollama[2557]: time=2025-01-23T15:19:17.805+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2908148708/runners
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.208+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.208+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.316+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.317+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.317+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 23 15:19:20 ishan-Legion ollama[2557]: time=2025-01-23T15:19:20.317+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Jan 23 15:55:28 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 23 15:55:28 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 23 15:55:28 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 23 15:55:28 ishan-Legion systemd[1]: ollama.service: Consumed 7.438s CPU time.
-- Boot 71a4086a5ffc4ae6a52b0152abd6f857 --
Jan 25 12:49:08 ishan-Legion systemd[1]: Started Ollama Service.
Jan 25 12:49:08 ishan-Legion ollama[2679]: 2025/01/25 12:49:08 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 25 12:49:08 ishan-Legion ollama[2679]: time=2025-01-25T12:49:08.838+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 25 12:49:08 ishan-Legion ollama[2679]: time=2025-01-25T12:49:08.839+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 25 12:49:08 ishan-Legion ollama[2679]: time=2025-01-25T12:49:08.839+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 25 12:49:08 ishan-Legion ollama[2679]: time=2025-01-25T12:49:08.840+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3204481249/runners
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.194+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.194+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.273+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.274+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.274+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 25 12:49:11 ishan-Legion ollama[2679]: time=2025-01-25T12:49:11.274+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 25 16:08:30 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 25 16:08:31 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 25 16:08:31 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 25 16:08:31 ishan-Legion systemd[1]: ollama.service: Consumed 7.305s CPU time.
-- Boot ed01b682e06b4db89bf8d274a400687a --
Jan 25 16:57:17 ishan-Legion systemd[1]: Started Ollama Service.
Jan 25 16:57:17 ishan-Legion ollama[2424]: 2025/01/25 16:57:17 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 25 16:57:17 ishan-Legion ollama[2424]: time=2025-01-25T16:57:17.337+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 25 16:57:17 ishan-Legion ollama[2424]: time=2025-01-25T16:57:17.337+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 25 16:57:17 ishan-Legion ollama[2424]: time=2025-01-25T16:57:17.337+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 25 16:57:17 ishan-Legion ollama[2424]: time=2025-01-25T16:57:17.338+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1141121375/runners
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.747+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.747+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.838+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.838+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.838+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 25 16:57:19 ishan-Legion ollama[2424]: time=2025-01-25T16:57:19.838+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
-- Boot 3b6286c125ec481bb41d9f61ea3ee0ed --
Jan 25 23:34:52 ishan-Legion systemd[1]: Started Ollama Service.
Jan 25 23:34:52 ishan-Legion ollama[2663]: 2025/01/25 23:34:52 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 25 23:34:52 ishan-Legion ollama[2663]: time=2025-01-25T23:34:52.250+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 25 23:34:52 ishan-Legion ollama[2663]: time=2025-01-25T23:34:52.250+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 25 23:34:52 ishan-Legion ollama[2663]: time=2025-01-25T23:34:52.251+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 25 23:34:52 ishan-Legion ollama[2663]: time=2025-01-25T23:34:52.251+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1056087461/runners
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.575+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.575+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.655+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.656+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.656+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 25 23:34:54 ishan-Legion ollama[2663]: time=2025-01-25T23:34:54.656+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
-- Boot e81ba993d6f54e04b36a16c7399e6bac --
Jan 25 23:36:20 ishan-Legion systemd[1]: Started Ollama Service.
Jan 25 23:36:21 ishan-Legion ollama[2491]: 2025/01/25 23:36:21 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 25 23:36:21 ishan-Legion ollama[2491]: time=2025-01-25T23:36:21.004+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 25 23:36:21 ishan-Legion ollama[2491]: time=2025-01-25T23:36:21.006+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 25 23:36:21 ishan-Legion ollama[2491]: time=2025-01-25T23:36:21.006+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 25 23:36:21 ishan-Legion ollama[2491]: time=2025-01-25T23:36:21.008+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2545086843/runners
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.387+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]"
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.387+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.474+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.474+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.474+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 25 23:36:23 ishan-Legion ollama[2491]: time=2025-01-25T23:36:23.474+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
-- Boot 07a96f38a7694b52b3850a818f7262f8 --
Jan 25 23:37:38 ishan-Legion systemd[1]: Started Ollama Service.
Jan 25 23:37:38 ishan-Legion ollama[2630]: 2025/01/25 23:37:38 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 25 23:37:38 ishan-Legion ollama[2630]: time=2025-01-25T23:37:38.803+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 25 23:37:38 ishan-Legion ollama[2630]: time=2025-01-25T23:37:38.804+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 25 23:37:38 ishan-Legion ollama[2630]: time=2025-01-25T23:37:38.804+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 25 23:37:38 ishan-Legion ollama[2630]: time=2025-01-25T23:37:38.805+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama4267521482/runners
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.120+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.121+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.199+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.200+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.200+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 25 23:37:41 ishan-Legion ollama[2630]: time=2025-01-25T23:37:41.200+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 26 06:11:08 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 26 06:11:08 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 26 06:11:08 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 26 06:11:08 ishan-Legion systemd[1]: ollama.service: Consumed 7.193s CPU time.
-- Boot 39ab2830e4ec4ec293c012359ceac04c --
Jan 26 10:00:55 ishan-Legion systemd[1]: Started Ollama Service.
Jan 26 10:00:55 ishan-Legion ollama[2656]: 2025/01/26 10:00:55 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 26 10:00:55 ishan-Legion ollama[2656]: time=2025-01-26T10:00:55.895+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 26 10:00:55 ishan-Legion ollama[2656]: time=2025-01-26T10:00:55.895+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 26 10:00:55 ishan-Legion ollama[2656]: time=2025-01-26T10:00:55.896+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 26 10:00:55 ishan-Legion ollama[2656]: time=2025-01-26T10:00:55.897+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2498779581/runners
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.243+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.243+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.331+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.331+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.331+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 26 10:00:58 ishan-Legion ollama[2656]: time=2025-01-26T10:00:58.331+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 26 21:41:43 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 26 21:41:43 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 26 21:41:43 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 26 21:41:43 ishan-Legion systemd[1]: ollama.service: Consumed 7.962s CPU time.
-- Boot 10ba0f734b6643d69283321926ee9f0e --
Jan 27 00:43:05 ishan-Legion systemd[1]: Started Ollama Service.
Jan 27 00:43:05 ishan-Legion ollama[2756]: 2025/01/27 00:43:05 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 27 00:43:05 ishan-Legion ollama[2756]: time=2025-01-27T00:43:05.835+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 27 00:43:05 ishan-Legion ollama[2756]: time=2025-01-27T00:43:05.836+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 27 00:43:05 ishan-Legion ollama[2756]: time=2025-01-27T00:43:05.837+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 27 00:43:05 ishan-Legion ollama[2756]: time=2025-01-27T00:43:05.837+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3994723303/runners
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.159+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.159+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.237+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.237+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.237+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 27 00:43:08 ishan-Legion ollama[2756]: time=2025-01-27T00:43:08.237+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 27 03:40:40 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 27 03:40:40 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 27 03:40:40 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 27 03:40:40 ishan-Legion systemd[1]: ollama.service: Consumed 7.051s CPU time.
-- Boot 305165e9fcce438387549a363d283138 --
Jan 27 11:51:58 ishan-Legion systemd[1]: Started Ollama Service.
Jan 27 11:51:58 ishan-Legion ollama[2665]: 2025/01/27 11:51:58 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 27 11:51:58 ishan-Legion ollama[2665]: time=2025-01-27T11:51:58.059+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 27 11:51:58 ishan-Legion ollama[2665]: time=2025-01-27T11:51:58.059+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 27 11:51:58 ishan-Legion ollama[2665]: time=2025-01-27T11:51:58.059+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 27 11:51:58 ishan-Legion ollama[2665]: time=2025-01-27T11:51:58.060+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1687785890/runners
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.382+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.383+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.490+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.490+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.490+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 27 11:52:00 ishan-Legion ollama[2665]: time=2025-01-27T11:52:00.490+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 27 13:53:56 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 27 13:53:56 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 27 13:53:56 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 27 13:53:56 ishan-Legion systemd[1]: ollama.service: Consumed 7.291s CPU time.
-- Boot 859d8621103f45d5867b655c6802565b --
Jan 27 14:55:24 ishan-Legion systemd[1]: Started Ollama Service.
Jan 27 14:55:24 ishan-Legion ollama[2589]: 2025/01/27 14:55:24 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 27 14:55:24 ishan-Legion ollama[2589]: time=2025-01-27T14:55:24.631+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 27 14:55:24 ishan-Legion ollama[2589]: time=2025-01-27T14:55:24.632+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 27 14:55:24 ishan-Legion ollama[2589]: time=2025-01-27T14:55:24.632+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 27 14:55:24 ishan-Legion ollama[2589]: time=2025-01-27T14:55:24.632+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3053403549/runners
Jan 27 14:55:26 ishan-Legion ollama[2589]: time=2025-01-27T14:55:26.957+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Jan 27 14:55:26 ishan-Legion ollama[2589]: time=2025-01-27T14:55:26.957+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 27 14:55:27 ishan-Legion ollama[2589]: time=2025-01-27T14:55:27.031+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 27 14:55:27 ishan-Legion ollama[2589]: time=2025-01-27T14:55:27.032+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 27 14:55:27 ishan-Legion ollama[2589]: time=2025-01-27T14:55:27.032+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 27 14:55:27 ishan-Legion ollama[2589]: time=2025-01-27T14:55:27.032+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Jan 27 22:46:19 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 27 22:46:19 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 27 22:46:19 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 27 22:46:19 ishan-Legion systemd[1]: ollama.service: Consumed 7.879s CPU time.
-- Boot 9f33483db0ba412abadc5fc2eb6d4242 --
Jan 27 23:41:37 ishan-Legion systemd[1]: Started Ollama Service.
Jan 27 23:41:37 ishan-Legion ollama[2711]: 2025/01/27 23:41:37 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 27 23:41:37 ishan-Legion ollama[2711]: time=2025-01-27T23:41:37.038+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 27 23:41:37 ishan-Legion ollama[2711]: time=2025-01-27T23:41:37.038+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 27 23:41:37 ishan-Legion ollama[2711]: time=2025-01-27T23:41:37.038+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 27 23:41:37 ishan-Legion ollama[2711]: time=2025-01-27T23:41:37.039+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3818094452/runners
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.372+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.372+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.446+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.447+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.447+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 27 23:41:39 ishan-Legion ollama[2711]: time=2025-01-27T23:41:39.447+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 28 04:16:25 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 28 04:16:25 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 28 04:16:25 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 28 04:16:25 ishan-Legion systemd[1]: ollama.service: Consumed 7.096s CPU time.
-- Boot d74ce31048414fe8bb62f1d20a6c0f5a --
Jan 28 15:44:11 ishan-Legion systemd[1]: Started Ollama Service.
Jan 28 15:44:11 ishan-Legion ollama[2587]: 2025/01/28 15:44:11 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 28 15:44:11 ishan-Legion ollama[2587]: time=2025-01-28T15:44:11.569+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 28 15:44:11 ishan-Legion ollama[2587]: time=2025-01-28T15:44:11.569+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 28 15:44:11 ishan-Legion ollama[2587]: time=2025-01-28T15:44:11.570+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 28 15:44:11 ishan-Legion ollama[2587]: time=2025-01-28T15:44:11.570+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1443581246/runners
Jan 28 15:44:13 ishan-Legion ollama[2587]: time=2025-01-28T15:44:13.926+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 28 15:44:13 ishan-Legion ollama[2587]: time=2025-01-28T15:44:13.930+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 28 15:44:14 ishan-Legion ollama[2587]: time=2025-01-28T15:44:14.021+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 28 15:44:14 ishan-Legion ollama[2587]: time=2025-01-28T15:44:14.022+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 28 15:44:14 ishan-Legion ollama[2587]: time=2025-01-28T15:44:14.022+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 28 15:44:14 ishan-Legion ollama[2587]: time=2025-01-28T15:44:14.022+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 28 15:45:42 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 28 15:45:43 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 28 15:45:43 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 28 15:45:43 ishan-Legion systemd[1]: ollama.service: Consumed 7.033s CPU time.
-- Boot c09efc9191e74f67a448da36a596a47b --
Jan 28 22:10:25 ishan-Legion systemd[1]: Started Ollama Service.
Jan 28 22:10:25 ishan-Legion ollama[2607]: 2025/01/28 22:10:25 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 28 22:10:25 ishan-Legion ollama[2607]: time=2025-01-28T22:10:25.458+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 28 22:10:25 ishan-Legion ollama[2607]: time=2025-01-28T22:10:25.459+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 28 22:10:25 ishan-Legion ollama[2607]: time=2025-01-28T22:10:25.459+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 28 22:10:25 ishan-Legion ollama[2607]: time=2025-01-28T22:10:25.460+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama17399850/runners
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.795+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.795+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.875+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.875+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.875+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 28 22:10:27 ishan-Legion ollama[2607]: time=2025-01-28T22:10:27.875+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 29 03:07:56 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 29 03:07:56 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 29 03:07:56 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 29 03:07:56 ishan-Legion systemd[1]: ollama.service: Consumed 7.124s CPU time.
-- Boot 89048a1e55a742d9b94682ac74ad2e90 --
Jan 29 11:20:04 ishan-Legion systemd[1]: Started Ollama Service.
Jan 29 11:20:04 ishan-Legion ollama[2509]: 2025/01/29 11:20:04 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 29 11:20:04 ishan-Legion ollama[2509]: time=2025-01-29T11:20:04.869+05:30 level=INFO source=images.go:782 msg="total blobs: 5"
Jan 29 11:20:04 ishan-Legion ollama[2509]: time=2025-01-29T11:20:04.870+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 29 11:20:04 ishan-Legion ollama[2509]: time=2025-01-29T11:20:04.870+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 29 11:20:04 ishan-Legion ollama[2509]: time=2025-01-29T11:20:04.871+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1434190216/runners
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.253+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.254+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.330+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.331+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.331+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 29 11:20:07 ishan-Legion ollama[2509]: time=2025-01-29T11:20:07.331+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 29 13:09:37 ishan-Legion ollama[2509]: [GIN] 2025/01/29 - 13:09:37 | 200 |     423.134s |       127.0.0.1 | HEAD     "/"
Jan 29 13:09:37 ishan-Legion ollama[2509]: [GIN] 2025/01/29 - 13:09:37 | 200 |    2.279167ms |       127.0.0.1 | GET      "/api/tags"
Jan 29 13:10:10 ishan-Legion ollama[2509]: [GIN] 2025/01/29 - 13:10:10 | 200 |      21.962s |       127.0.0.1 | HEAD     "/"
Jan 29 13:10:10 ishan-Legion ollama[2509]: [GIN] 2025/01/29 - 13:10:10 | 200 |   16.995675ms |       127.0.0.1 | DELETE   "/api/delete"
Jan 29 14:56:55 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 29 14:56:55 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 29 14:56:55 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 29 14:56:55 ishan-Legion systemd[1]: ollama.service: Consumed 7.208s CPU time.
-- Boot 2b560a7aff52446fadee1bf4a8f219c9 --
Jan 30 00:15:10 ishan-Legion systemd[1]: Started Ollama Service.
Jan 30 00:15:10 ishan-Legion ollama[2689]: 2025/01/30 00:15:10 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 30 00:15:10 ishan-Legion ollama[2689]: time=2025-01-30T00:15:10.694+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 30 00:15:10 ishan-Legion ollama[2689]: time=2025-01-30T00:15:10.695+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 30 00:15:10 ishan-Legion ollama[2689]: time=2025-01-30T00:15:10.695+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 30 00:15:10 ishan-Legion ollama[2689]: time=2025-01-30T00:15:10.695+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama155631386/runners
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.031+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.031+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.108+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.109+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.109+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 30 00:15:13 ishan-Legion ollama[2689]: time=2025-01-30T00:15:13.109+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 30 00:17:17 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 30 00:17:17 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 30 00:17:17 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 30 00:17:17 ishan-Legion systemd[1]: ollama.service: Consumed 7.038s CPU time.
-- Boot ddd992af762648e799b031792c6b9295 --
Jan 30 00:17:48 ishan-Legion systemd[1]: Started Ollama Service.
Jan 30 00:17:48 ishan-Legion ollama[2729]: 2025/01/30 00:17:48 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 30 00:17:48 ishan-Legion ollama[2729]: time=2025-01-30T00:17:48.176+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 30 00:17:48 ishan-Legion ollama[2729]: time=2025-01-30T00:17:48.176+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 30 00:17:48 ishan-Legion ollama[2729]: time=2025-01-30T00:17:48.177+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 30 00:17:48 ishan-Legion ollama[2729]: time=2025-01-30T00:17:48.177+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2121960436/runners
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.529+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11]"
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.529+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.604+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.604+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.604+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 30 00:17:50 ishan-Legion ollama[2729]: time=2025-01-30T00:17:50.604+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 30 01:48:50 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 30 01:48:50 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 30 01:48:50 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 30 01:48:50 ishan-Legion systemd[1]: ollama.service: Consumed 6.940s CPU time.
-- Boot d7f72b3de5334181844d18fad4d8a5ad --
Jan 30 10:42:53 ishan-Legion systemd[1]: Started Ollama Service.
Jan 30 10:42:53 ishan-Legion ollama[2678]: 2025/01/30 10:42:53 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 30 10:42:53 ishan-Legion ollama[2678]: time=2025-01-30T10:42:53.897+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 30 10:42:53 ishan-Legion ollama[2678]: time=2025-01-30T10:42:53.897+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 30 10:42:53 ishan-Legion ollama[2678]: time=2025-01-30T10:42:53.898+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 30 10:42:53 ishan-Legion ollama[2678]: time=2025-01-30T10:42:53.898+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1795062018/runners
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.212+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.212+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.288+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.288+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.288+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 30 10:42:56 ishan-Legion ollama[2678]: time=2025-01-30T10:42:56.288+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 30 15:52:53 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 30 15:52:54 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 30 15:52:54 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 30 15:52:54 ishan-Legion systemd[1]: ollama.service: Consumed 7.337s CPU time.
-- Boot e8348157b8144585ba6643f2515be2fd --
Jan 30 16:45:24 ishan-Legion systemd[1]: Started Ollama Service.
Jan 30 16:45:24 ishan-Legion ollama[2641]: 2025/01/30 16:45:24 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 30 16:45:24 ishan-Legion ollama[2641]: time=2025-01-30T16:45:24.823+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 30 16:45:24 ishan-Legion ollama[2641]: time=2025-01-30T16:45:24.823+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 30 16:45:24 ishan-Legion ollama[2641]: time=2025-01-30T16:45:24.824+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 30 16:45:24 ishan-Legion ollama[2641]: time=2025-01-30T16:45:24.824+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2796170191/runners
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.181+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.181+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.263+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.263+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.263+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 30 16:45:27 ishan-Legion ollama[2641]: time=2025-01-30T16:45:27.263+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 31 04:13:35 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 31 04:13:36 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 31 04:13:36 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 31 04:13:36 ishan-Legion systemd[1]: ollama.service: Consumed 7.828s CPU time.
-- Boot 2f257290f40e468196a5b33b62c78d82 --
Jan 31 15:43:53 ishan-Legion systemd[1]: Started Ollama Service.
Jan 31 15:43:53 ishan-Legion ollama[2651]: 2025/01/31 15:43:53 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 31 15:43:53 ishan-Legion ollama[2651]: time=2025-01-31T15:43:53.242+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 31 15:43:53 ishan-Legion ollama[2651]: time=2025-01-31T15:43:53.242+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 31 15:43:53 ishan-Legion ollama[2651]: time=2025-01-31T15:43:53.242+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 31 15:43:53 ishan-Legion ollama[2651]: time=2025-01-31T15:43:53.243+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1529785279/runners
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.564+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.564+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.641+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.641+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.641+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 31 15:43:55 ishan-Legion ollama[2651]: time=2025-01-31T15:43:55.641+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Jan 31 15:47:00 ishan-Legion systemd[1]: Stopping Ollama Service...
Jan 31 15:47:00 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Jan 31 15:47:00 ishan-Legion systemd[1]: Stopped Ollama Service.
Jan 31 15:47:00 ishan-Legion systemd[1]: ollama.service: Consumed 6.883s CPU time.
-- Boot e3186e50816a4f8e90c9cce8db3b6280 --
Jan 31 15:47:37 ishan-Legion systemd[1]: Started Ollama Service.
Jan 31 15:47:37 ishan-Legion ollama[2652]: 2025/01/31 15:47:37 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Jan 31 15:47:37 ishan-Legion ollama[2652]: time=2025-01-31T15:47:37.161+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Jan 31 15:47:37 ishan-Legion ollama[2652]: time=2025-01-31T15:47:37.161+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Jan 31 15:47:37 ishan-Legion ollama[2652]: time=2025-01-31T15:47:37.161+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Jan 31 15:47:37 ishan-Legion ollama[2652]: time=2025-01-31T15:47:37.162+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1627329086/runners
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.466+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.466+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.540+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.540+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.540+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Jan 31 15:47:39 ishan-Legion ollama[2652]: time=2025-01-31T15:47:39.540+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 01 11:13:47 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 01 11:13:48 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 01 11:13:48 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 01 11:13:48 ishan-Legion systemd[1]: ollama.service: Consumed 9.065s CPU time.
-- Boot 4791cea10d6648dbaa8fb456187c83ac --
Feb 01 11:42:58 ishan-Legion systemd[1]: Started Ollama Service.
Feb 01 11:42:58 ishan-Legion ollama[2583]: 2025/02/01 11:42:58 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 01 11:42:58 ishan-Legion ollama[2583]: time=2025-02-01T11:42:58.353+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 01 11:42:58 ishan-Legion ollama[2583]: time=2025-02-01T11:42:58.353+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 01 11:42:58 ishan-Legion ollama[2583]: time=2025-02-01T11:42:58.353+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 01 11:42:58 ishan-Legion ollama[2583]: time=2025-02-01T11:42:58.354+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1776566052/runners
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.783+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.783+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.859+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.859+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.859+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 01 11:43:00 ishan-Legion ollama[2583]: time=2025-02-01T11:43:00.859+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.4 GiB"
Feb 01 22:56:45 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 01 22:56:46 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 01 22:56:46 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 01 22:56:46 ishan-Legion systemd[1]: ollama.service: Consumed 8.281s CPU time.
-- Boot 325bd0bec5f64ce29484a1368b18c6de --
Feb 02 22:59:55 ishan-Legion systemd[1]: Started Ollama Service.
Feb 02 22:59:55 ishan-Legion ollama[2575]: 2025/02/02 22:59:55 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 02 22:59:55 ishan-Legion ollama[2575]: time=2025-02-02T22:59:55.880+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 02 22:59:55 ishan-Legion ollama[2575]: time=2025-02-02T22:59:55.880+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 02 22:59:55 ishan-Legion ollama[2575]: time=2025-02-02T22:59:55.880+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 02 22:59:55 ishan-Legion ollama[2575]: time=2025-02-02T22:59:55.881+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2127730550/runners
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.301+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.301+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.380+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.381+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.381+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 02 22:59:58 ishan-Legion ollama[2575]: time=2025-02-02T22:59:58.381+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.4 GiB"
Feb 02 23:00:53 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 02 23:00:53 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 02 23:00:53 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 02 23:00:53 ishan-Legion systemd[1]: ollama.service: Consumed 7.155s CPU time.
-- Boot 75d58bc32e894a43b70bc9b9b4d8f3c8 --
Feb 02 23:01:34 ishan-Legion systemd[1]: Started Ollama Service.
Feb 02 23:01:34 ishan-Legion ollama[2477]: 2025/02/02 23:01:34 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 02 23:01:34 ishan-Legion ollama[2477]: time=2025-02-02T23:01:34.810+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 02 23:01:34 ishan-Legion ollama[2477]: time=2025-02-02T23:01:34.810+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 02 23:01:34 ishan-Legion ollama[2477]: time=2025-02-02T23:01:34.810+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 02 23:01:34 ishan-Legion ollama[2477]: time=2025-02-02T23:01:34.811+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama580660974/runners
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.230+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.230+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.315+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.315+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.316+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 02 23:01:37 ishan-Legion ollama[2477]: time=2025-02-02T23:01:37.316+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.4 GiB"
Feb 03 01:43:00 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 03 01:43:01 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 03 01:43:01 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 03 01:43:01 ishan-Legion systemd[1]: ollama.service: Consumed 7.329s CPU time.
-- Boot 5bdb48fd10374c3a84d60b47518764ff --
Feb 03 20:41:46 ishan-Legion systemd[1]: Started Ollama Service.
Feb 03 20:41:46 ishan-Legion ollama[2562]: 2025/02/03 20:41:46 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 03 20:41:46 ishan-Legion ollama[2562]: time=2025-02-03T20:41:46.439+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 03 20:41:46 ishan-Legion ollama[2562]: time=2025-02-03T20:41:46.439+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 03 20:41:46 ishan-Legion ollama[2562]: time=2025-02-03T20:41:46.439+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 03 20:41:46 ishan-Legion ollama[2562]: time=2025-02-03T20:41:46.440+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2789759106/runners
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.780+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.781+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.856+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.856+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.856+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 03 20:41:48 ishan-Legion ollama[2562]: time=2025-02-03T20:41:48.857+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 04 01:17:52 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 04 01:17:52 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 04 01:17:52 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 04 01:17:52 ishan-Legion systemd[1]: ollama.service: Consumed 7.300s CPU time.
-- Boot ae480def91044dec9b2ccff5488e4706 --
Feb 04 07:48:58 ishan-Legion systemd[1]: Started Ollama Service.
Feb 04 07:48:58 ishan-Legion ollama[2468]: 2025/02/04 07:48:58 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 04 07:48:58 ishan-Legion ollama[2468]: time=2025-02-04T07:48:58.742+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 04 07:48:58 ishan-Legion ollama[2468]: time=2025-02-04T07:48:58.742+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 04 07:48:58 ishan-Legion ollama[2468]: time=2025-02-04T07:48:58.743+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 04 07:48:58 ishan-Legion ollama[2468]: time=2025-02-04T07:48:58.743+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama4146841035/runners
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.120+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.121+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.196+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.196+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.196+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 04 07:49:01 ishan-Legion ollama[2468]: time=2025-02-04T07:49:01.196+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.2 GiB"
Feb 04 07:52:35 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 04 07:52:36 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 04 07:52:36 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 04 07:52:36 ishan-Legion systemd[1]: ollama.service: Consumed 7.134s CPU time.
-- Boot 7e40d005be67468480b5b93ae64ce2a7 --
Feb 04 09:36:34 ishan-Legion systemd[1]: Started Ollama Service.
Feb 04 09:36:34 ishan-Legion ollama[2540]: 2025/02/04 09:36:34 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 04 09:36:34 ishan-Legion ollama[2540]: time=2025-02-04T09:36:34.803+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 04 09:36:34 ishan-Legion ollama[2540]: time=2025-02-04T09:36:34.803+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 04 09:36:34 ishan-Legion ollama[2540]: time=2025-02-04T09:36:34.803+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 04 09:36:34 ishan-Legion ollama[2540]: time=2025-02-04T09:36:34.804+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3395992108/runners
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.218+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.218+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.297+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.298+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.298+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 04 09:36:37 ishan-Legion ollama[2540]: time=2025-02-04T09:36:37.298+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.4 GiB"
Feb 04 10:25:58 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 04 10:25:58 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 04 10:25:58 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 04 10:25:58 ishan-Legion systemd[1]: ollama.service: Consumed 7.293s CPU time.
-- Boot b8230e3320b44c19a2bd8c8c97e64526 --
Feb 06 00:48:44 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 00:48:44 ishan-Legion ollama[3944]: 2025/02/06 00:48:44 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 00:48:44 ishan-Legion ollama[3944]: time=2025-02-06T00:48:44.589+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 00:48:44 ishan-Legion ollama[3944]: time=2025-02-06T00:48:44.590+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 00:48:44 ishan-Legion ollama[3944]: time=2025-02-06T00:48:44.590+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 00:48:44 ishan-Legion ollama[3944]: time=2025-02-06T00:48:44.590+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama830651075/runners
Feb 06 00:48:46 ishan-Legion ollama[3944]: time=2025-02-06T00:48:46.904+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]"
Feb 06 00:48:46 ishan-Legion ollama[3944]: time=2025-02-06T00:48:46.904+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 00:48:47 ishan-Legion ollama[3944]: time=2025-02-06T00:48:47.003+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 00:48:47 ishan-Legion ollama[3944]: time=2025-02-06T00:48:47.003+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 00:48:47 ishan-Legion ollama[3944]: time=2025-02-06T00:48:47.003+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 00:48:47 ishan-Legion ollama[3944]: time=2025-02-06T00:48:47.003+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.3 GiB"
Feb 06 00:51:21 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 00:51:21 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 00:51:21 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 00:51:21 ishan-Legion systemd[1]: ollama.service: Consumed 6.911s CPU time.
-- Boot 02118e589588456fa08f1a0180e45d9e --
Feb 06 00:51:48 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 00:51:48 ishan-Legion ollama[2629]: 2025/02/06 00:51:48 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 00:51:48 ishan-Legion ollama[2629]: time=2025-02-06T00:51:48.887+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 00:51:48 ishan-Legion ollama[2629]: time=2025-02-06T00:51:48.888+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 00:51:48 ishan-Legion ollama[2629]: time=2025-02-06T00:51:48.888+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 00:51:48 ishan-Legion ollama[2629]: time=2025-02-06T00:51:48.889+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1351340821/runners
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.265+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.265+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.339+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.339+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.339+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 00:51:51 ishan-Legion ollama[2629]: time=2025-02-06T00:51:51.339+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 06 01:21:17 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 01:21:17 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 01:21:17 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 01:21:17 ishan-Legion systemd[1]: ollama.service: Consumed 7.069s CPU time.
-- Boot 209f5944d479479daa09213ecc2f1118 --
Feb 06 01:51:59 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 01:51:59 ishan-Legion ollama[2686]: 2025/02/06 01:51:59 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 01:51:59 ishan-Legion ollama[2686]: time=2025-02-06T01:51:59.201+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 01:51:59 ishan-Legion ollama[2686]: time=2025-02-06T01:51:59.202+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 01:51:59 ishan-Legion ollama[2686]: time=2025-02-06T01:51:59.202+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 01:51:59 ishan-Legion ollama[2686]: time=2025-02-06T01:51:59.203+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama144383610/runners
Feb 06 01:52:01 ishan-Legion ollama[2686]: time=2025-02-06T01:52:01.928+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Feb 06 01:52:01 ishan-Legion ollama[2686]: time=2025-02-06T01:52:01.929+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 01:52:02 ishan-Legion ollama[2686]: time=2025-02-06T01:52:02.005+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 01:52:02 ishan-Legion ollama[2686]: time=2025-02-06T01:52:02.006+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 01:52:02 ishan-Legion ollama[2686]: time=2025-02-06T01:52:02.006+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 01:52:02 ishan-Legion ollama[2686]: time=2025-02-06T01:52:02.006+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
-- Boot 683f469d462c46f1a933b2911c65738d --
Feb 06 08:23:17 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 08:23:17 ishan-Legion ollama[2691]: 2025/02/06 08:23:17 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 08:23:17 ishan-Legion ollama[2691]: time=2025-02-06T08:23:17.947+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 08:23:17 ishan-Legion ollama[2691]: time=2025-02-06T08:23:17.947+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 08:23:17 ishan-Legion ollama[2691]: time=2025-02-06T08:23:17.947+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 08:23:17 ishan-Legion ollama[2691]: time=2025-02-06T08:23:17.948+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1470595724/runners
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.322+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.322+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.439+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.439+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.440+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 08:23:20 ishan-Legion ollama[2691]: time=2025-02-06T08:23:20.440+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 06 08:24:14 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 08:24:14 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 08:24:14 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 08:24:14 ishan-Legion systemd[1]: ollama.service: Consumed 7.204s CPU time.
-- Boot b22c8cbe28a74a3389bed57f46d4989e --
Feb 06 08:24:45 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 08:24:45 ishan-Legion ollama[2504]: 2025/02/06 08:24:45 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 08:24:45 ishan-Legion ollama[2504]: time=2025-02-06T08:24:45.741+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 08:24:45 ishan-Legion ollama[2504]: time=2025-02-06T08:24:45.741+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 08:24:45 ishan-Legion ollama[2504]: time=2025-02-06T08:24:45.741+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 08:24:45 ishan-Legion ollama[2504]: time=2025-02-06T08:24:45.742+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama301046007/runners
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.124+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60102 cpu cpu_avx]"
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.124+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.199+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.200+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.200+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 08:24:48 ishan-Legion ollama[2504]: time=2025-02-06T08:24:48.200+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 06 08:51:55 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 08:51:56 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 08:51:56 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 08:51:56 ishan-Legion systemd[1]: ollama.service: Consumed 7.186s CPU time.
-- Boot 16f5d7ec3c8243d7a541c6119eb68d29 --
Feb 06 11:46:26 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 11:46:26 ishan-Legion ollama[3879]: 2025/02/06 11:46:26 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 11:46:26 ishan-Legion ollama[3879]: time=2025-02-06T11:46:26.491+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 11:46:26 ishan-Legion ollama[3879]: time=2025-02-06T11:46:26.491+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 11:46:26 ishan-Legion ollama[3879]: time=2025-02-06T11:46:26.491+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 11:46:26 ishan-Legion ollama[3879]: time=2025-02-06T11:46:26.492+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3853400022/runners
Feb 06 11:46:28 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.005+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries []"
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.005+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.123+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.124+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.124+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 11:46:29 ishan-Legion ollama[3879]: time=2025-02-06T11:46:29.124+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.6 GiB"
Feb 06 11:46:29 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 11:46:29 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 11:46:29 ishan-Legion systemd[1]: ollama.service: Consumed 7.227s CPU time.
-- Boot 9fe06b25bfe84c29b16bc8066ba2de19 --
Feb 06 11:47:50 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 11:47:50 ishan-Legion ollama[3852]: 2025/02/06 11:47:50 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 11:47:50 ishan-Legion ollama[3852]: time=2025-02-06T11:47:50.192+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 11:47:50 ishan-Legion ollama[3852]: time=2025-02-06T11:47:50.192+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 11:47:50 ishan-Legion ollama[3852]: time=2025-02-06T11:47:50.192+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 11:47:50 ishan-Legion ollama[3852]: time=2025-02-06T11:47:50.193+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama789466785/runners
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.654+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.655+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.793+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.794+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.794+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 11:47:52 ishan-Legion ollama[3852]: time=2025-02-06T11:47:52.794+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.3 GiB"
-- Boot 33df61566d314e4b995e5fdda3a08a60 --
Feb 06 14:18:33 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 14:18:33 ishan-Legion ollama[2704]: 2025/02/06 14:18:33 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 14:18:33 ishan-Legion ollama[2704]: time=2025-02-06T14:18:33.963+05:30 level=INFO source=images.go:782 msg="total blobs: 0"
Feb 06 14:18:33 ishan-Legion ollama[2704]: time=2025-02-06T14:18:33.963+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 14:18:33 ishan-Legion ollama[2704]: time=2025-02-06T14:18:33.963+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 14:18:33 ishan-Legion ollama[2704]: time=2025-02-06T14:18:33.964+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama3383558217/runners
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.294+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cuda_v11 rocm_v60102 cpu cpu_avx cpu_avx2]"
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.294+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.369+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.369+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.369+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 14:18:36 ishan-Legion ollama[2704]: time=2025-02-06T14:18:36.369+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="7.5 GiB"
Feb 06 20:23:23 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:23:23 | 200 |     961.434s |       127.0.0.1 | HEAD     "/"
Feb 06 20:23:23 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:23:23 | 200 |     355.701s |       127.0.0.1 | GET      "/api/tags"
Feb 06 20:23:28 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:23:28 | 200 |      22.242s |       127.0.0.1 | HEAD     "/"
Feb 06 20:23:28 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:23:28 | 404 |     177.875s |       127.0.0.1 | POST     "/api/show"
Feb 06 20:23:30 ishan-Legion ollama[2704]: time=2025-02-06T20:23:30.177+05:30 level=INFO source=download.go:175 msg="downloading 5ff0abeeac1d in 65 139 MB part(s)"
Feb 06 20:24:11 ishan-Legion ollama[2704]: time=2025-02-06T20:24:11.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 8 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:24:19 ishan-Legion ollama[2704]: time=2025-02-06T20:24:19.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 22 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:24:28 ishan-Legion ollama[2704]: time=2025-02-06T20:24:28.487+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 49 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:25:03 ishan-Legion ollama[2704]: time=2025-02-06T20:25:03.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 2 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:25:31 ishan-Legion ollama[2704]: time=2025-02-06T20:25:31.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 9 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:25:55 ishan-Legion ollama[2704]: time=2025-02-06T20:25:55.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 6 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:27:48 ishan-Legion ollama[2704]: time=2025-02-06T20:27:48.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 10 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:28:54 ishan-Legion ollama[2704]: time=2025-02-06T20:28:54.486+05:30 level=INFO source=download.go:370 msg="5ff0abeeac1d part 53 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
Feb 06 20:29:22 ishan-Legion ollama[2704]: time=2025-02-06T20:29:22.125+05:30 level=INFO source=download.go:175 msg="downloading 22091531faf0 in 1 705 B part(s)"
Feb 06 20:29:23 ishan-Legion ollama[2704]: time=2025-02-06T20:29:23.870+05:30 level=INFO source=download.go:175 msg="downloading 4bb71764481f in 1 13 KB part(s)"
Feb 06 20:29:25 ishan-Legion ollama[2704]: time=2025-02-06T20:29:25.716+05:30 level=INFO source=download.go:175 msg="downloading 1c8f573e830c in 1 1.1 KB part(s)"
Feb 06 20:29:27 ishan-Legion ollama[2704]: time=2025-02-06T20:29:27.624+05:30 level=INFO source=download.go:175 msg="downloading 19f2fb9e8bc6 in 1 32 B part(s)"
Feb 06 20:29:29 ishan-Legion ollama[2704]: time=2025-02-06T20:29:29.484+05:30 level=INFO source=download.go:175 msg="downloading 34488e453cfe in 1 568 B part(s)"
Feb 06 20:29:38 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:29:38 | 200 |          6m9s |       127.0.0.1 | POST     "/api/pull"
Feb 06 20:29:38 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:29:38 | 200 |    8.804619ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.161+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=15 layers.split="" memory.available="[6.9 GiB]" memory.required.full="11.9 GiB" memory.required.partial="6.9 GiB" memory.required.kv="2.6 GiB" memory.required.allocations="[6.9 GiB]" memory.weights.total="10.7 GiB" memory.weights.repeating="10.5 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="364.0 MiB" memory.graph.partial="483.4 MiB"
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.162+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 10240 --batch-size 512 --embedding --log-disable --n-gpu-layers 15 --no-mmap --parallel 1 --port 36841"
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.163+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.163+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.164+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:29:38 ishan-Legion ollama[44223]: INFO [main] build info | build=1 commit="1e6f655" tid="134931149418496" timestamp=1738853978
Feb 06 20:29:38 ishan-Legion ollama[44223]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="134931149418496" timestamp=1738853978 total_threads=16
Feb 06 20:29:38 ishan-Legion ollama[44223]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="36841" tid="134931149418496" timestamp=1738853978
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:29:38 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:29:38 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:29:38 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:29:38 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:29:38 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:29:38 ishan-Legion ollama[2704]: time=2025-02-06T20:29:38.414+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:29:38 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:29:39 ishan-Legion ollama[2704]: time=2025-02-06T20:29:39.869+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:29:40 ishan-Legion ollama[2704]: time=2025-02-06T20:29:40.300+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:29:40 ishan-Legion ollama[2704]: llm_load_tensors: offloading 15 repeating layers to GPU
Feb 06 20:29:40 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 15/28 layers to GPU
Feb 06 20:29:40 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  3776.04 MiB
Feb 06 20:29:40 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  4712.73 MiB
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 10240
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:29:45 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_kv_cache_init:  CUDA_Host KV buffer size =  1200.00 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  1500.00 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 2700.00 MiB, K (f16): 1620.00 MiB, V (f16): 1080.00 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =   483.38 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    30.01 MiB
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:29:46 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 192
Feb 06 20:29:46 ishan-Legion ollama[2704]: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED
Feb 06 20:29:46 ishan-Legion ollama[2704]:   current device: 0, in function cublas_handle at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda/common.cuh:644
Feb 06 20:29:46 ishan-Legion ollama[2704]:   cublasCreate_v2(&cublas_handles[device])
Feb 06 20:29:46 ishan-Legion ollama[2704]: /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:101: CUDA error
Feb 06 20:29:46 ishan-Legion ollama[2704]: Could not attach to process.  If your uid matches the uid of the target
Feb 06 20:29:46 ishan-Legion ollama[2704]: process, check the setting of /proc/sys/kernel/yama/ptrace_scope, or try
Feb 06 20:29:46 ishan-Legion ollama[2704]: again as the root user.  For more details, see /etc/sysctl.d/10-ptrace.conf
Feb 06 20:29:46 ishan-Legion ollama[2704]: ptrace: Inappropriate ioctl for device.
Feb 06 20:29:46 ishan-Legion ollama[2704]: No stack.
Feb 06 20:29:46 ishan-Legion ollama[2704]: The program is not being run.
Feb 06 20:29:47 ishan-Legion ollama[2704]: time=2025-02-06T20:29:47.273+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:29:47 ishan-Legion ollama[2704]: time=2025-02-06T20:29:47.525+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:29:48 ishan-Legion ollama[2704]: time=2025-02-06T20:29:48.277+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="llama runner process has terminated: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\n  current device: 0, in function cublas_handle at /go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda/common.cuh:644\n  cublasCreate_v2(&cublas_handles[device])\n/go/src/github.com/ollama/ollama/llm/llama.cpp/ggml/src/ggml-cuda.cu:101: CUDA error"
Feb 06 20:29:48 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:29:48 | 500 | 10.203887847s |       127.0.0.1 | POST     "/api/chat"
Feb 06 20:30:12 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:30:12 | 200 |      20.158s |       127.0.0.1 | HEAD     "/"
Feb 06 20:30:12 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:30:12 | 200 |    9.195932ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:30:12 ishan-Legion ollama[2704]: time=2025-02-06T20:30:12.852+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=14 layers.split="" memory.available="[6.8 GiB]" memory.required.full="11.9 GiB" memory.required.partial="6.5 GiB" memory.required.kv="2.6 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="10.7 GiB" memory.weights.repeating="10.5 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="364.0 MiB" memory.graph.partial="483.4 MiB"
Feb 06 20:30:12 ishan-Legion ollama[2704]: time=2025-02-06T20:30:12.853+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 10240 --batch-size 512 --embedding --log-disable --n-gpu-layers 14 --no-mmap --parallel 1 --port 33245"
Feb 06 20:30:12 ishan-Legion ollama[2704]: time=2025-02-06T20:30:12.854+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:30:12 ishan-Legion ollama[2704]: time=2025-02-06T20:30:12.854+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:30:12 ishan-Legion ollama[2704]: time=2025-02-06T20:30:12.854+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:30:12 ishan-Legion ollama[44730]: INFO [main] build info | build=1 commit="1e6f655" tid="130780048457728" timestamp=1738854012
Feb 06 20:30:12 ishan-Legion ollama[44730]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="130780048457728" timestamp=1738854012 total_threads=16
Feb 06 20:30:12 ishan-Legion ollama[44730]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33245" tid="130780048457728" timestamp=1738854012
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:30:12 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:30:12 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:30:13 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:30:13 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:30:13 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:30:13 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:30:13 ishan-Legion ollama[2704]: time=2025-02-06T20:30:13.105+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:13 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:30:14 ishan-Legion ollama[2704]: time=2025-02-06T20:30:14.559+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:30:14 ishan-Legion ollama[2704]: time=2025-02-06T20:30:14.848+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:14 ishan-Legion ollama[2704]: llm_load_tensors: offloading 14 repeating layers to GPU
Feb 06 20:30:14 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 14/28 layers to GPU
Feb 06 20:30:14 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  4090.22 MiB
Feb 06 20:30:14 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  4398.55 MiB
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 10240
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:30:19 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:30:20 ishan-Legion ollama[2704]: time=2025-02-06T20:30:20.066+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:30:20 ishan-Legion ollama[2704]: time=2025-02-06T20:30:20.331+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_kv_cache_init:  CUDA_Host KV buffer size =  1300.00 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  1400.00 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 2700.00 MiB, K (f16): 1620.00 MiB, V (f16): 1080.00 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =   483.38 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    30.01 MiB
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:30:20 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 208
Feb 06 20:30:21 ishan-Legion ollama[44730]: INFO [main] model loaded | tid="130780048457728" timestamp=1738854021
Feb 06 20:30:21 ishan-Legion ollama[2704]: time=2025-02-06T20:30:21.336+05:30 level=INFO source=server.go:632 msg="llama runner started in 8.48 seconds"
Feb 06 20:30:21 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:30:21 | 200 |  8.578878859s |       127.0.0.1 | POST     "/api/chat"
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.010+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=14 layers.split="" memory.available="[6.8 GiB]" memory.required.full="11.9 GiB" memory.required.partial="6.5 GiB" memory.required.kv="2.6 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="10.7 GiB" memory.weights.repeating="10.5 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="364.0 MiB" memory.graph.partial="483.4 MiB"
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.011+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 10240 --batch-size 512 --embedding --log-disable --n-gpu-layers 14 --no-mmap --parallel 1 --port 42425"
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.011+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.011+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.011+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:30:25 ishan-Legion ollama[44886]: INFO [main] build info | build=1 commit="1e6f655" tid="138607870009344" timestamp=1738854025
Feb 06 20:30:25 ishan-Legion ollama[44886]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="138607870009344" timestamp=1738854025 total_threads=16
Feb 06 20:30:25 ishan-Legion ollama[44886]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="42425" tid="138607870009344" timestamp=1738854025
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:30:25 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:30:25 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:30:25 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:30:25 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:30:25 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:30:25 ishan-Legion ollama[2704]: time=2025-02-06T20:30:25.262+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:25 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:30:26 ishan-Legion ollama[2704]: time=2025-02-06T20:30:26.716+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:30:26 ishan-Legion ollama[2704]: time=2025-02-06T20:30:26.999+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:27 ishan-Legion ollama[2704]: llm_load_tensors: offloading 14 repeating layers to GPU
Feb 06 20:30:27 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 14/28 layers to GPU
Feb 06 20:30:27 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  4090.22 MiB
Feb 06 20:30:27 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  4398.55 MiB
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 10240
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_kv_cache_init:  CUDA_Host KV buffer size =  1300.00 MiB
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  1400.00 MiB
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 2700.00 MiB, K (f16): 1620.00 MiB, V (f16): 1080.00 MiB
Feb 06 20:30:31 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:30:32 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =   483.38 MiB
Feb 06 20:30:32 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    30.01 MiB
Feb 06 20:30:32 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:30:32 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 208
Feb 06 20:30:32 ishan-Legion ollama[44886]: INFO [main] model loaded | tid="138607870009344" timestamp=1738854032
Feb 06 20:30:33 ishan-Legion ollama[2704]: time=2025-02-06T20:30:33.025+05:30 level=INFO source=server.go:632 msg="llama runner started in 8.01 seconds"
Feb 06 20:30:33 ishan-Legion ollama[2704]: check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
Feb 06 20:30:34 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:30:34 | 200 | 10.514329537s |       127.0.0.1 | POST     "/api/chat"
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.077+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=14 layers.split="" memory.available="[6.8 GiB]" memory.required.full="11.9 GiB" memory.required.partial="6.5 GiB" memory.required.kv="2.6 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="10.7 GiB" memory.weights.repeating="10.5 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="364.0 MiB" memory.graph.partial="483.4 MiB"
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.078+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 10240 --batch-size 512 --embedding --log-disable --n-gpu-layers 14 --parallel 1 --port 40431"
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.078+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.078+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.078+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:30:50 ishan-Legion ollama[45545]: INFO [main] build info | build=1 commit="1e6f655" tid="133784062058496" timestamp=1738854050
Feb 06 20:30:50 ishan-Legion ollama[45545]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="133784062058496" timestamp=1738854050 total_threads=16
Feb 06 20:30:50 ishan-Legion ollama[45545]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="40431" tid="133784062058496" timestamp=1738854050
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:30:50 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:30:50 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:30:50 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:30:50 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:30:50 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:30:50 ishan-Legion ollama[2704]: time=2025-02-06T20:30:50.329+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:30:50 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:30:52 ishan-Legion ollama[2704]: llm_load_tensors: offloading 14 repeating layers to GPU
Feb 06 20:30:52 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 14/28 layers to GPU
Feb 06 20:30:52 ishan-Legion ollama[2704]: llm_load_tensors:        CPU buffer size =  4421.57 MiB
Feb 06 20:30:52 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  4398.55 MiB
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 10240
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:30:52 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_kv_cache_init:  CUDA_Host KV buffer size =  1300.00 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  1400.00 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 2700.00 MiB, K (f16): 1620.00 MiB, V (f16): 1080.00 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =   483.38 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    30.01 MiB
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:30:53 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 208
Feb 06 20:30:54 ishan-Legion ollama[45545]: INFO [main] model loaded | tid="133784062058496" timestamp=1738854054
Feb 06 20:30:54 ishan-Legion ollama[2704]: time=2025-02-06T20:30:54.345+05:30 level=INFO source=server.go:632 msg="llama runner started in 4.27 seconds"
Feb 06 20:30:54 ishan-Legion ollama[2704]: check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
Feb 06 20:31:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:31:01 | 200 | 13.031551499s |       127.0.0.1 | POST     "/api/chat"
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.005+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=14 layers.split="" memory.available="[6.8 GiB]" memory.required.full="11.9 GiB" memory.required.partial="6.5 GiB" memory.required.kv="2.6 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="10.7 GiB" memory.weights.repeating="10.5 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="364.0 MiB" memory.graph.partial="483.4 MiB"
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.006+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 10240 --batch-size 512 --embedding --log-disable --n-gpu-layers 14 --parallel 1 --port 46111"
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.006+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.006+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.006+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:31:25 ishan-Legion ollama[48618]: INFO [main] build info | build=1 commit="1e6f655" tid="129047092289536" timestamp=1738854085
Feb 06 20:31:25 ishan-Legion ollama[48618]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129047092289536" timestamp=1738854085 total_threads=16
Feb 06 20:31:25 ishan-Legion ollama[48618]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="46111" tid="129047092289536" timestamp=1738854085
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:31:25 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:31:25 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:31:25 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:31:25 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:31:25 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:31:25 ishan-Legion ollama[2704]: time=2025-02-06T20:31:25.258+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_tensors: offloading 14 repeating layers to GPU
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 14/28 layers to GPU
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_tensors:        CPU buffer size =  4421.57 MiB
Feb 06 20:31:25 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  4398.55 MiB
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 10240
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:31:26 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_kv_cache_init:  CUDA_Host KV buffer size =  1300.00 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  1400.00 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 2700.00 MiB, K (f16): 1620.00 MiB, V (f16): 1080.00 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =   483.38 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    30.01 MiB
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:31:27 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 208
Feb 06 20:31:27 ishan-Legion ollama[48618]: INFO [main] model loaded | tid="129047092289536" timestamp=1738854087
Feb 06 20:31:28 ishan-Legion ollama[2704]: time=2025-02-06T20:31:28.044+05:30 level=INFO source=server.go:632 msg="llama runner started in 3.04 seconds"
Feb 06 20:31:28 ishan-Legion ollama[2704]: check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?
Feb 06 20:31:52 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:31:52 | 200 | 27.812339719s |       127.0.0.1 | POST     "/api/chat"
Feb 06 20:35:00 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:00 | 200 |   10.218588ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:00 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:00 | 200 |   13.833266ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:14 | 200 |    9.858166ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:14 | 200 |    9.824212ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:14 | 200 |    9.824883ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:14 | 200 |   10.143708ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:14 | 200 |   10.513839ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:35:14 ishan-Legion ollama[2704]: cuda driver library failed to get device context 2time=2025-02-06T20:35:14.376+05:30 level=WARN source=gpu.go:403 msg="error looking up nvidia GPU memory"
Feb 06 20:35:14 ishan-Legion ollama[2704]: cuda driver library failed to get device context 2time=2025-02-06T20:35:14.728+05:30 level=WARN source=gpu.go:403 msg="error looking up nvidia GPU memory"
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.281+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.283+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 46215"
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.283+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.283+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.283+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:15 ishan-Legion ollama[2704]: time=2025-02-06T20:35:15.284+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:15 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:15 | 499 |  1.006184653s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:15 ishan-Legion ollama[61860]: INFO [main] build info | build=1 commit="1e6f655" tid="130201266921472" timestamp=1738854315
Feb 06 20:35:15 ishan-Legion ollama[61860]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="130201266921472" timestamp=1738854315 total_threads=16
Feb 06 20:35:15 ishan-Legion ollama[61860]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="46215" tid="130201266921472" timestamp=1738854315
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:15 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.413+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.120303339 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.577+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.578+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 45063"
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.578+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.578+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.578+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.578+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:20 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:20 | 499 |  5.645219282s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:20 ishan-Legion ollama[62384]: INFO [main] build info | build=1 commit="1e6f655" tid="126954266812416" timestamp=1738854320
Feb 06 20:35:20 ishan-Legion ollama[62384]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="126954266812416" timestamp=1738854320 total_threads=16
Feb 06 20:35:20 ishan-Legion ollama[62384]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="45063" tid="126954266812416" timestamp=1738854320
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:20 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.653+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.369607746 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:20 ishan-Legion ollama[2704]: time=2025-02-06T20:35:20.904+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.620048023 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.037+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.038+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 40785"
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.039+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.039+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.039+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:35:21 ishan-Legion ollama[62426]: INFO [main] build info | build=1 commit="1e6f655" tid="126887374487552" timestamp=1738854321
Feb 06 20:35:21 ishan-Legion ollama[62426]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="126887374487552" timestamp=1738854321 total_threads=16
Feb 06 20:35:21 ishan-Legion ollama[62426]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="40785" tid="126887374487552" timestamp=1738854321
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:21 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:35:21 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:35:21 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:35:21 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:35:21 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:35:21 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.289+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:21 ishan-Legion ollama[2704]: time=2025-02-06T20:35:21.289+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:21 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:21 | 499 |  1.031874213s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.359+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.070346478 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.512+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.513+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37243"
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.513+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.513+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.513+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.513+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:26 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:26 | 499 |   5.42924559s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:26 ishan-Legion ollama[62598]: INFO [main] build info | build=1 commit="1e6f655" tid="131232694226944" timestamp=1738854326
Feb 06 20:35:26 ishan-Legion ollama[62598]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="131232694226944" timestamp=1738854326 total_threads=16
Feb 06 20:35:26 ishan-Legion ollama[62598]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37243" tid="131232694226944" timestamp=1738854326
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:26 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.610+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.320549523 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:26 ishan-Legion ollama[2704]: time=2025-02-06T20:35:26.860+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.570604769 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.002+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.003+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 33901"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.003+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.003+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.003+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.003+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:27 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:27 | 499 |  911.525885ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:27 ishan-Legion ollama[62627]: INFO [main] build info | build=1 commit="1e6f655" tid="128397167013888" timestamp=1738854327
Feb 06 20:35:27 ishan-Legion ollama[62627]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="128397167013888" timestamp=1738854327 total_threads=16
Feb 06 20:35:27 ishan-Legion ollama[62627]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33901" tid="128397167013888" timestamp=1738854327
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.464+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.465+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 36333"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.465+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.465+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.465+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:27 ishan-Legion ollama[2704]: time=2025-02-06T20:35:27.465+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:27 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:27 | 499 |  842.584995ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:27 ishan-Legion ollama[62661]: INFO [main] build info | build=1 commit="1e6f655" tid="128496173744128" timestamp=1738854327
Feb 06 20:35:27 ishan-Legion ollama[62661]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="128496173744128" timestamp=1738854327 total_threads=16
Feb 06 20:35:27 ishan-Legion ollama[62661]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="36333" tid="128496173744128" timestamp=1738854327
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:27 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.539+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.073852314 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.700+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.701+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 39107"
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.701+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.701+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.701+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.701+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:32 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:32 | 499 |  5.300515207s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:32 ishan-Legion ollama[62813]: INFO [main] build info | build=1 commit="1e6f655" tid="140251959009280" timestamp=1738854332
Feb 06 20:35:32 ishan-Legion ollama[62813]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="140251959009280" timestamp=1738854332 total_threads=16
Feb 06 20:35:32 ishan-Legion ollama[62813]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="39107" tid="140251959009280" timestamp=1738854332
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:32 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:32 ishan-Legion ollama[2704]: time=2025-02-06T20:35:32.789+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.324080585 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:33 ishan-Legion ollama[2704]: time=2025-02-06T20:35:33.039+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.573845509 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.420+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.421+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 36657"
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.421+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.421+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.422+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:37 ishan-Legion ollama[2704]: time=2025-02-06T20:35:37.422+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:37 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:37 | 499 |   5.73679933s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:37 ishan-Legion ollama[63008]: INFO [main] build info | build=1 commit="1e6f655" tid="129332419162112" timestamp=1738854337
Feb 06 20:35:37 ishan-Legion ollama[63008]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129332419162112" timestamp=1738854337 total_threads=16
Feb 06 20:35:37 ishan-Legion ollama[63008]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="36657" tid="129332419162112" timestamp=1738854337
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:37 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.658+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.659+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 41239"
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.659+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.659+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.659+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:35:39 ishan-Legion ollama[63070]: INFO [main] build info | build=1 commit="1e6f655" tid="139967549083648" timestamp=1738854339
Feb 06 20:35:39 ishan-Legion ollama[63070]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="139967549083648" timestamp=1738854339 total_threads=16
Feb 06 20:35:39 ishan-Legion ollama[63070]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="41239" tid="139967549083648" timestamp=1738854339
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:39 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:35:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:35:39 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:35:39 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:35:39 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:35:39 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:35:39 ishan-Legion ollama[2704]: time=2025-02-06T20:35:39.920+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:35:40 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:35:40 ishan-Legion ollama[2704]: time=2025-02-06T20:35:40.170+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:40 ishan-Legion ollama[2704]: time=2025-02-06T20:35:40.170+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:40 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:40 | 499 |  3.014237222s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.243+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.07229693 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.394+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.404+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 33077"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.404+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.404+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.404+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.404+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:45 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:45 | 499 |  5.317581569s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:45 ishan-Legion ollama[63277]: INFO [main] build info | build=1 commit="1e6f655" tid="125020664905728" timestamp=1738854345
Feb 06 20:35:45 ishan-Legion ollama[63277]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="125020664905728" timestamp=1738854345 total_threads=16
Feb 06 20:35:45 ishan-Legion ollama[63277]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33077" tid="125020664905728" timestamp=1738854345
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.492+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.321444299 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.742+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.572009985 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.876+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.877+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 38685"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.877+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.877+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:45 ishan-Legion ollama[2704]: time=2025-02-06T20:35:45.878+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:35:45 ishan-Legion ollama[63304]: INFO [main] build info | build=1 commit="1e6f655" tid="131918884646912" timestamp=1738854345
Feb 06 20:35:45 ishan-Legion ollama[63304]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="131918884646912" timestamp=1738854345 total_threads=16
Feb 06 20:35:45 ishan-Legion ollama[63304]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="38685" tid="131918884646912" timestamp=1738854345
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:45 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:35:46 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:35:46 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:35:46 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:35:46 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:35:46 ishan-Legion ollama[2704]: time=2025-02-06T20:35:46.130+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:35:46 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:35:47 ishan-Legion ollama[2704]: time=2025-02-06T20:35:47.585+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:35:48 ishan-Legion ollama[2704]: time=2025-02-06T20:35:48.825+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:35:49 ishan-Legion ollama[2704]: time=2025-02-06T20:35:49.077+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:49 ishan-Legion ollama[2704]: time=2025-02-06T20:35:49.077+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:49 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:49 | 499 |  5.515753457s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:49 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:35:49 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:35:49 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:35:49 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.081+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.004318116 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.232+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.233+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 44067"
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.242+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.242+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.242+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.242+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:54 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:54 | 499 |  5.416221122s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:35:54 ishan-Legion ollama[63450]: INFO [main] build info | build=1 commit="1e6f655" tid="126029094932480" timestamp=1738854354
Feb 06 20:35:54 ishan-Legion ollama[63450]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="126029094932480" timestamp=1738854354 total_threads=16
Feb 06 20:35:54 ishan-Legion ollama[63450]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="44067" tid="126029094932480" timestamp=1738854354
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:54 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.331+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.25376329 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:54 ishan-Legion ollama[2704]: time=2025-02-06T20:35:54.581+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.50395612 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:35:57 ishan-Legion ollama[2704]: time=2025-02-06T20:35:57.957+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:35:57 ishan-Legion ollama[2704]: time=2025-02-06T20:35:57.958+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 43923"
Feb 06 20:35:57 ishan-Legion ollama[2704]: time=2025-02-06T20:35:57.958+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:35:57 ishan-Legion ollama[2704]: time=2025-02-06T20:35:57.958+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:35:57 ishan-Legion ollama[2704]: time=2025-02-06T20:35:57.958+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:35:57 ishan-Legion ollama[63533]: INFO [main] build info | build=1 commit="1e6f655" tid="135854423724032" timestamp=1738854357
Feb 06 20:35:57 ishan-Legion ollama[63533]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="135854423724032" timestamp=1738854357 total_threads=16
Feb 06 20:35:57 ishan-Legion ollama[63533]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="43923" tid="135854423724032" timestamp=1738854357
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:35:57 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:35:58 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:35:58 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:35:58 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:35:58 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:35:58 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:35:58 ishan-Legion ollama[2704]: time=2025-02-06T20:35:58.209+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:35:58 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:35:59 ishan-Legion ollama[2704]: time=2025-02-06T20:35:59.221+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:35:59 ishan-Legion ollama[2704]: time=2025-02-06T20:35:59.221+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:35:59 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:35:59 | 499 |  9.810906381s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.295+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.073099285 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.443+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.444+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 39755"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.445+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.445+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.445+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.445+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:36:04 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:36:04 | 499 |  5.406375975s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:36:04 ishan-Legion ollama[63713]: INFO [main] build info | build=1 commit="1e6f655" tid="140105987108864" timestamp=1738854364
Feb 06 20:36:04 ishan-Legion ollama[63713]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="140105987108864" timestamp=1738854364 total_threads=16
Feb 06 20:36:04 ishan-Legion ollama[63713]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="39755" tid="140105987108864" timestamp=1738854364
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.544+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.32267094 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.794+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.572617355 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.893+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.903+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 42713"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.903+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.903+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:36:04 ishan-Legion ollama[2704]: time=2025-02-06T20:36:04.904+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:36:04 ishan-Legion ollama[63746]: INFO [main] build info | build=1 commit="1e6f655" tid="135034185457664" timestamp=1738854364
Feb 06 20:36:04 ishan-Legion ollama[63746]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="135034185457664" timestamp=1738854364 total_threads=16
Feb 06 20:36:04 ishan-Legion ollama[63746]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="42713" tid="135034185457664" timestamp=1738854364
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:36:04 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:36:05 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:36:05 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:36:05 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:36:05 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:36:05 ishan-Legion ollama[2704]: time=2025-02-06T20:36:05.155+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:36:05 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:36:06 ishan-Legion ollama[2704]: time=2025-02-06T20:36:06.610+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:36:08 ishan-Legion ollama[2704]: time=2025-02-06T20:36:08.222+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:36:08 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:36:08 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:36:08 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:36:08 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:36:13 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:36:13 ishan-Legion ollama[2704]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:36:18 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 336
Feb 06 20:36:50 ishan-Legion ollama[63746]: INFO [main] model loaded | tid="135034185457664" timestamp=1738854410
Feb 06 20:36:50 ishan-Legion ollama[2704]: time=2025-02-06T20:36:50.902+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:36:57 ishan-Legion ollama[2704]: time=2025-02-06T20:36:57.920+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:36:57 ishan-Legion ollama[2704]: time=2025-02-06T20:36:57.926+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:36:57 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:36:57 | 499 | 57.327309918s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:01 | 200 |   53.768591ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:37:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:01 | 200 |   52.927367ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:37:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:01 | 200 |   50.382655ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:37:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:01 | 200 |   56.727288ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:37:01 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:01 | 200 |   53.679753ms |       127.0.0.1 | POST     "/api/show"
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.247+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.249+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 38037"
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.250+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.250+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.250+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:37:23 ishan-Legion ollama[65727]: INFO [main] build info | build=1 commit="1e6f655" tid="130147355553792" timestamp=1738854443
Feb 06 20:37:23 ishan-Legion ollama[65727]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="130147355553792" timestamp=1738854443 total_threads=16
Feb 06 20:37:23 ishan-Legion ollama[65727]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="38037" tid="130147355553792" timestamp=1738854443
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:23 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.501+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:37:23 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:37:23 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:37:23 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:37:23 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:37:23 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.752+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:23 ishan-Legion ollama[2704]: time=2025-02-06T20:37:23.752+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:23 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:23 | 499 |  705.805129ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:28 ishan-Legion ollama[2704]: time=2025-02-06T20:37:28.881+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.128973549 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.034+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.035+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 33377"
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.035+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.035+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.035+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.035+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:29 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:29 | 499 |  5.421401318s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:29 ishan-Legion ollama[65900]: INFO [main] build info | build=1 commit="1e6f655" tid="140697363808256" timestamp=1738854449
Feb 06 20:37:29 ishan-Legion ollama[65900]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="140697363808256" timestamp=1738854449 total_threads=16
Feb 06 20:37:29 ishan-Legion ollama[65900]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33377" tid="140697363808256" timestamp=1738854449
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:29 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.132+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.3798384949999996 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:29 ishan-Legion ollama[2704]: time=2025-02-06T20:37:29.381+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.6289881600000005 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.280+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.281+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 43893"
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.281+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.281+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.281+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:30 ishan-Legion ollama[2704]: time=2025-02-06T20:37:30.281+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:30 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:30 | 499 |  1.435652856s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:30 ishan-Legion ollama[65963]: INFO [main] build info | build=1 commit="1e6f655" tid="124436894543872" timestamp=1738854450
Feb 06 20:37:30 ishan-Legion ollama[65963]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="124436894543872" timestamp=1738854450 total_threads=16
Feb 06 20:37:30 ishan-Legion ollama[65963]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="43893" tid="124436894543872" timestamp=1738854450
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:30 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.069+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.070+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 46297"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.071+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.071+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.071+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.071+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:31 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:31 | 499 |  828.245698ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:31 ishan-Legion ollama[66016]: INFO [main] build info | build=1 commit="1e6f655" tid="133828017291264" timestamp=1738854451
Feb 06 20:37:31 ishan-Legion ollama[66016]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="133828017291264" timestamp=1738854451 total_threads=16
Feb 06 20:37:31 ishan-Legion ollama[66016]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="46297" tid="133828017291264" timestamp=1738854451
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.573+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.574+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37355"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.575+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.575+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.575+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:31 ishan-Legion ollama[2704]: time=2025-02-06T20:37:31.575+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:31 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:31 | 499 |  854.751221ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:31 ishan-Legion ollama[66054]: INFO [main] build info | build=1 commit="1e6f655" tid="135792039649280" timestamp=1738854451
Feb 06 20:37:31 ishan-Legion ollama[66054]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="135792039649280" timestamp=1738854451 total_threads=16
Feb 06 20:37:31 ishan-Legion ollama[66054]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37355" tid="135792039649280" timestamp=1738854451
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:31 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.644+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.069130032 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.790+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.791+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 40257"
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.791+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.791+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.791+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.791+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:36 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:36 | 499 |  5.288164119s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:36 ishan-Legion ollama[66233]: INFO [main] build info | build=1 commit="1e6f655" tid="127894057971712" timestamp=1738854456
Feb 06 20:37:36 ishan-Legion ollama[66233]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="127894057971712" timestamp=1738854456 total_threads=16
Feb 06 20:37:36 ishan-Legion ollama[66233]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="40257" tid="127894057971712" timestamp=1738854456
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:36 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:36 ishan-Legion ollama[2704]: time=2025-02-06T20:37:36.895+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.319909412 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:37 ishan-Legion ollama[2704]: time=2025-02-06T20:37:37.144+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.569205623 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.271+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.272+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 32833"
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.273+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.273+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.273+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:37:38 ishan-Legion ollama[66282]: INFO [main] build info | build=1 commit="1e6f655" tid="123616494444544" timestamp=1738854458
Feb 06 20:37:38 ishan-Legion ollama[66282]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="123616494444544" timestamp=1738854458 total_threads=16
Feb 06 20:37:38 ishan-Legion ollama[66282]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="32833" tid="123616494444544" timestamp=1738854458
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:38 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:37:38 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:37:38 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:37:38 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:37:38 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:37:38 ishan-Legion ollama[2704]: time=2025-02-06T20:37:38.524+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:37:38 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:37:39 ishan-Legion ollama[2704]: time=2025-02-06T20:37:39.978+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:37:41 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:37:41 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:37:41 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:37:41 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:37:41 ishan-Legion ollama[2704]: time=2025-02-06T20:37:41.584+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:37:42 ishan-Legion ollama[2704]: time=2025-02-06T20:37:42.337+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:42 ishan-Legion ollama[2704]: time=2025-02-06T20:37:42.337+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:42 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:42 | 499 |  7.642562106s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.428+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.091073398 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.579+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.580+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37125"
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.580+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.580+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.580+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.580+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:47 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:47 | 499 |  5.318282738s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:47 ishan-Legion ollama[66463]: INFO [main] build info | build=1 commit="1e6f655" tid="126918470496256" timestamp=1738854467
Feb 06 20:37:47 ishan-Legion ollama[66463]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="126918470496256" timestamp=1738854467 total_threads=16
Feb 06 20:37:47 ishan-Legion ollama[66463]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37125" tid="126918470496256" timestamp=1738854467
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:47 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.678+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.341127525 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:47 ishan-Legion ollama[2704]: time=2025-02-06T20:37:47.927+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.590599999 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.611+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.612+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 32913"
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.613+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.613+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.613+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:51 ishan-Legion ollama[2704]: time=2025-02-06T20:37:51.613+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:51 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:51 | 499 |  6.146197426s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:37:51 ishan-Legion ollama[66571]: INFO [main] build info | build=1 commit="1e6f655" tid="130503690858496" timestamp=1738854471
Feb 06 20:37:51 ishan-Legion ollama[66571]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="130503690858496" timestamp=1738854471 total_threads=16
Feb 06 20:37:51 ishan-Legion ollama[66571]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="32913" tid="130503690858496" timestamp=1738854471
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:51 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.325+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.327+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 42231"
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.327+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.327+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.328+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:37:53 ishan-Legion ollama[66621]: INFO [main] build info | build=1 commit="1e6f655" tid="136096098377728" timestamp=1738854473
Feb 06 20:37:53 ishan-Legion ollama[66621]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="136096098377728" timestamp=1738854473 total_threads=16
Feb 06 20:37:53 ishan-Legion ollama[66621]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="42231" tid="136096098377728" timestamp=1738854473
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:37:53 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:37:53 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:37:53 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:37:53 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:37:53 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:37:53 ishan-Legion ollama[2704]: time=2025-02-06T20:37:53.578+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:37:53 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:37:55 ishan-Legion ollama[2704]: time=2025-02-06T20:37:55.033+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:37:56 ishan-Legion ollama[2704]: time=2025-02-06T20:37:56.285+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:37:56 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:37:56 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:37:56 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:37:56 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:37:57 ishan-Legion ollama[2704]: time=2025-02-06T20:37:57.540+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:37:57 ishan-Legion ollama[2704]: time=2025-02-06T20:37:57.540+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:37:57 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:37:57 | 499 |  9.180973423s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.625+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0852599 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.757+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.758+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 40735"
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.759+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.759+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.759+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.759+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:38:02 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:38:02 | 499 |  5.301976887s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:38:02 ishan-Legion ollama[66890]: INFO [main] build info | build=1 commit="1e6f655" tid="123942211727360" timestamp=1738854482
Feb 06 20:38:02 ishan-Legion ollama[66890]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="123942211727360" timestamp=1738854482 total_threads=16
Feb 06 20:38:02 ishan-Legion ollama[66890]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="40735" tid="123942211727360" timestamp=1738854482
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:38:02 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:38:02 ishan-Legion ollama[2704]: time=2025-02-06T20:38:02.874+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.334528851 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.125+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.584863535 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.983+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.983+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 34631"
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.984+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.984+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:38:03 ishan-Legion ollama[2704]: time=2025-02-06T20:38:03.984+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:38:03 ishan-Legion ollama[66923]: INFO [main] build info | build=1 commit="1e6f655" tid="133105586872320" timestamp=1738854483
Feb 06 20:38:03 ishan-Legion ollama[66923]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="133105586872320" timestamp=1738854483 total_threads=16
Feb 06 20:38:03 ishan-Legion ollama[66923]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="34631" tid="133105586872320" timestamp=1738854483
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:38:04 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:38:04 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:38:04 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:38:04 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:38:04 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:38:04 ishan-Legion ollama[2704]: time=2025-02-06T20:38:04.235+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:38:04 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:38:05 ishan-Legion ollama[2704]: time=2025-02-06T20:38:05.689+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:38:06 ishan-Legion ollama[2704]: time=2025-02-06T20:38:06.971+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:38:07 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:38:07 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:38:07 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:38:07 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:38:11 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:38:11 ishan-Legion ollama[2704]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:38:17 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 336
Feb 06 20:38:26 ishan-Legion ollama[2704]: time=2025-02-06T20:38:26.395+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:38:26 ishan-Legion ollama[2704]: time=2025-02-06T20:38:26.397+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:38:26 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:38:26 | 499 | 24.403851768s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.700+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.704+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 34705"
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.705+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.705+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.705+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:38:28 ishan-Legion ollama[2704]: time=2025-02-06T20:38:28.705+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:38:28 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:38:28 | 499 |  2.544226813s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:38:28 ishan-Legion ollama[67283]: INFO [main] build info | build=1 commit="1e6f655" tid="138581141843968" timestamp=1738854508
Feb 06 20:38:28 ishan-Legion ollama[67283]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="138581141843968" timestamp=1738854508 total_threads=16
Feb 06 20:38:28 ishan-Legion ollama[67283]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="34705" tid="138581141843968" timestamp=1738854508
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:38:28 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:38:33 ishan-Legion ollama[2704]: time=2025-02-06T20:38:33.838+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.133086121 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:33 ishan-Legion ollama[2704]: time=2025-02-06T20:38:33.998+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.9 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.011+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 33761"
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.012+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.012+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.013+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:38:34 ishan-Legion ollama[67406]: INFO [main] build info | build=1 commit="1e6f655" tid="136178225283072" timestamp=1738854514
Feb 06 20:38:34 ishan-Legion ollama[67406]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="136178225283072" timestamp=1738854514 total_threads=16
Feb 06 20:38:34 ishan-Legion ollama[67406]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33761" tid="136178225283072" timestamp=1738854514
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:38:34 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.088+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.38290366 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:38:34 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:38:34 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:38:34 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:38:34 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.264+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:38:34 ishan-Legion ollama[2704]: time=2025-02-06T20:38:34.338+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.63278966 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:38:34 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:38:35 ishan-Legion ollama[2704]: time=2025-02-06T20:38:35.718+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:38:37 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:38:37 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:38:37 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:38:37 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:38:37 ishan-Legion ollama[2704]: time=2025-02-06T20:38:37.324+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:38:41 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:38:41 ishan-Legion ollama[2704]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:38:46 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 336
Feb 06 20:39:17 ishan-Legion ollama[67406]: INFO [main] model loaded | tid="136178225283072" timestamp=1738854557
Feb 06 20:39:17 ishan-Legion ollama[2704]: time=2025-02-06T20:39:17.781+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:39:27 ishan-Legion ollama[2704]: time=2025-02-06T20:39:27.193+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:39:27 ishan-Legion ollama[2704]: time=2025-02-06T20:39:27.445+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:27 ishan-Legion ollama[2704]: time=2025-02-06T20:39:27.447+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:27 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:27 | 499 |          1m0s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.716+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.721+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 39737"
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.722+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.722+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.722+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:29 ishan-Legion ollama[2704]: time=2025-02-06T20:39:29.722+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:29 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:29 | 499 |  2.506510384s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:29 ishan-Legion ollama[68094]: INFO [main] build info | build=1 commit="1e6f655" tid="135245581471744" timestamp=1738854569
Feb 06 20:39:29 ishan-Legion ollama[68094]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="135245581471744" timestamp=1738854569 total_threads=16
Feb 06 20:39:29 ishan-Legion ollama[68094]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="39737" tid="135245581471744" timestamp=1738854569
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:29 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.260+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.262+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 41529"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.262+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.262+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.262+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.262+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:30 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:30 | 499 |  2.688394472s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:30 ishan-Legion ollama[68140]: INFO [main] build info | build=1 commit="1e6f655" tid="136163646472192" timestamp=1738854570
Feb 06 20:39:30 ishan-Legion ollama[68140]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="136163646472192" timestamp=1738854570 total_threads=16
Feb 06 20:39:30 ishan-Legion ollama[68140]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="41529" tid="136163646472192" timestamp=1738854570
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.797+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.799+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 41141"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.799+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.799+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.799+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:30 ishan-Legion ollama[2704]: time=2025-02-06T20:39:30.799+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:30 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:30 | 499 |  933.327826ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:30 ishan-Legion ollama[68167]: INFO [main] build info | build=1 commit="1e6f655" tid="134140375031808" timestamp=1738854570
Feb 06 20:39:30 ishan-Legion ollama[68167]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="134140375031808" timestamp=1738854570 total_threads=16
Feb 06 20:39:30 ishan-Legion ollama[68167]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="41141" tid="134140375031808" timestamp=1738854570
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:30 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.277+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.278+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 38897"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.278+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.278+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.278+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.278+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:33 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:33 | 499 |  2.862473892s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:33 ishan-Legion ollama[68242]: INFO [main] build info | build=1 commit="1e6f655" tid="131687243796480" timestamp=1738854573
Feb 06 20:39:33 ishan-Legion ollama[68242]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="131687243796480" timestamp=1738854573 total_threads=16
Feb 06 20:39:33 ishan-Legion ollama[68242]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="38897" tid="131687243796480" timestamp=1738854573
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.794+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.795+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 42551"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.796+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.796+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:33 ishan-Legion ollama[2704]: time=2025-02-06T20:39:33.796+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:39:33 ishan-Legion ollama[68277]: INFO [main] build info | build=1 commit="1e6f655" tid="129331698798592" timestamp=1738854573
Feb 06 20:39:33 ishan-Legion ollama[68277]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129331698798592" timestamp=1738854573 total_threads=16
Feb 06 20:39:33 ishan-Legion ollama[68277]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="42551" tid="129331698798592" timestamp=1738854573
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:33 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:39:34 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:39:34 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:39:34 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:39:34 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:39:34 ishan-Legion ollama[2704]: time=2025-02-06T20:39:34.046+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:39:34 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:39:34 ishan-Legion ollama[2704]: time=2025-02-06T20:39:34.297+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:34 ishan-Legion ollama[2704]: time=2025-02-06T20:39:34.297+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:34 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:34 | 499 |  1.037893104s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.365+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.067640909 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.492+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.493+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37521"
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.494+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.494+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.494+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:39:39 ishan-Legion ollama[68403]: INFO [main] build info | build=1 commit="1e6f655" tid="133656474820608" timestamp=1738854579
Feb 06 20:39:39 ishan-Legion ollama[68403]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="133656474820608" timestamp=1738854579 total_threads=16
Feb 06 20:39:39 ishan-Legion ollama[68403]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37521" tid="133656474820608" timestamp=1738854579
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:39 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.616+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.318493433 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:39:39 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:39:39 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:39:39 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:39:39 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.745+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:39:39 ishan-Legion ollama[2704]: time=2025-02-06T20:39:39.866+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.568365139 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:39 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:39:41 ishan-Legion ollama[2704]: time=2025-02-06T20:39:41.200+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:39:42 ishan-Legion ollama[2704]: time=2025-02-06T20:39:42.504+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:39:42 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:39:42 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:39:42 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:39:42 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:39:44 ishan-Legion ollama[2704]: time=2025-02-06T20:39:44.010+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:44 ishan-Legion ollama[2704]: time=2025-02-06T20:39:44.011+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:44 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:44 | 499 |  9.875249225s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.133+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.12258926 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.274+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.275+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 38473"
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.275+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.275+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.275+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.275+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:49 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:49 | 499 |  5.359415662s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:49 ishan-Legion ollama[68633]: INFO [main] build info | build=1 commit="1e6f655" tid="138022547341312" timestamp=1738854589
Feb 06 20:39:49 ishan-Legion ollama[68633]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="138022547341312" timestamp=1738854589 total_threads=16
Feb 06 20:39:49 ishan-Legion ollama[68633]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="38473" tid="138022547341312" timestamp=1738854589
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:49 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.383+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.372139293 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:49 ishan-Legion ollama[2704]: time=2025-02-06T20:39:49.633+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.621778074 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.268+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.269+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37375"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.270+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.270+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.270+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.270+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:52 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:52 | 499 |   4.21664498s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:52 ishan-Legion ollama[68723]: INFO [main] build info | build=1 commit="1e6f655" tid="139430944288768" timestamp=1738854592
Feb 06 20:39:52 ishan-Legion ollama[68723]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="139430944288768" timestamp=1738854592 total_threads=16
Feb 06 20:39:52 ishan-Legion ollama[68723]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37375" tid="139430944288768" timestamp=1738854592
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.739+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.740+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 34889"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.741+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.741+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.741+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:39:52 ishan-Legion ollama[68760]: INFO [main] build info | build=1 commit="1e6f655" tid="125527873855488" timestamp=1738854592
Feb 06 20:39:52 ishan-Legion ollama[68760]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="125527873855488" timestamp=1738854592 total_threads=16
Feb 06 20:39:52 ishan-Legion ollama[68760]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="34889" tid="125527873855488" timestamp=1738854592
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:39:52 ishan-Legion ollama[2704]: time=2025-02-06T20:39:52.992+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:39:52 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:39:52 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:39:52 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:39:52 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:39:52 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:39:53 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:39:53 ishan-Legion ollama[2704]: time=2025-02-06T20:39:53.242+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:53 ishan-Legion ollama[2704]: time=2025-02-06T20:39:53.242+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:53 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:53 | 499 |  1.072194164s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.305+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.062971438 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.449+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.9 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.450+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 35491"
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.451+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.451+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.451+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.451+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:39:58 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:39:58 | 499 |  5.343909735s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:39:58 ishan-Legion ollama[68932]: INFO [main] build info | build=1 commit="1e6f655" tid="123708765904896" timestamp=1738854598
Feb 06 20:39:58 ishan-Legion ollama[68932]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="123708765904896" timestamp=1738854598 total_threads=16
Feb 06 20:39:58 ishan-Legion ollama[68932]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="35491" tid="123708765904896" timestamp=1738854598
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:58 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.555+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.313494121 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:58 ishan-Legion ollama[2704]: time=2025-02-06T20:39:58.805+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.563553917 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.401+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.402+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 39035"
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.403+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.403+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.403+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:39:59 ishan-Legion ollama[68972]: INFO [main] build info | build=1 commit="1e6f655" tid="123932213297152" timestamp=1738854599
Feb 06 20:39:59 ishan-Legion ollama[68972]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="123932213297152" timestamp=1738854599 total_threads=16
Feb 06 20:39:59 ishan-Legion ollama[68972]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="39035" tid="123932213297152" timestamp=1738854599
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:39:59 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:39:59 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:39:59 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:39:59 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:39:59 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:39:59 ishan-Legion ollama[2704]: time=2025-02-06T20:39:59.654+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:39:59 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:40:01 ishan-Legion ollama[2704]: time=2025-02-06T20:40:01.109+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:40:02 ishan-Legion ollama[2704]: time=2025-02-06T20:40:02.271+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:40:02 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:40:02 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:40:02 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:40:02 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:40:05 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:40:05 ishan-Legion ollama[2704]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:40:10 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 336
Feb 06 20:40:28 ishan-Legion ollama[2704]: time=2025-02-06T20:40:28.400+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:28 ishan-Legion ollama[2704]: time=2025-02-06T20:40:28.403+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:28 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:28 | 499 | 32.529149778s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.687+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.693+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 38149"
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.693+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.693+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.693+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:30 ishan-Legion ollama[2704]: time=2025-02-06T20:40:30.693+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:30 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:30 | 499 |  2.469118071s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:30 ishan-Legion ollama[69661]: INFO [main] build info | build=1 commit="1e6f655" tid="123893918453760" timestamp=1738854630
Feb 06 20:40:30 ishan-Legion ollama[69661]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="123893918453760" timestamp=1738854630 total_threads=16
Feb 06 20:40:30 ishan-Legion ollama[69661]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="38149" tid="123893918453760" timestamp=1738854630
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:30 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.764+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.07003878 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.899+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.900+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 42147"
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.901+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.901+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.901+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:35 ishan-Legion ollama[2704]: time=2025-02-06T20:40:35.901+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:35 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:35 | 499 |  7.475944973s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:35 ishan-Legion ollama[69821]: INFO [main] build info | build=1 commit="1e6f655" tid="129436680413184" timestamp=1738854635
Feb 06 20:40:35 ishan-Legion ollama[69821]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129436680413184" timestamp=1738854635 total_threads=16
Feb 06 20:40:35 ishan-Legion ollama[69821]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="42147" tid="129436680413184" timestamp=1738854635
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:35 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.013+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.319892131 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.264+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.570347072 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.865+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.866+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 35095"
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.866+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.867+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.867+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:36 ishan-Legion ollama[2704]: time=2025-02-06T20:40:36.867+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:36 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:36 | 499 |  1.519614818s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:36 ishan-Legion ollama[69862]: INFO [main] build info | build=1 commit="1e6f655" tid="129117652152320" timestamp=1738854636
Feb 06 20:40:36 ishan-Legion ollama[69862]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129117652152320" timestamp=1738854636 total_threads=16
Feb 06 20:40:36 ishan-Legion ollama[69862]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="35095" tid="129117652152320" timestamp=1738854636
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:36 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:41 ishan-Legion ollama[2704]: time=2025-02-06T20:40:41.938+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.071710944 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.137+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.138+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 41551"
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.139+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.139+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.139+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.139+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:42 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:42 | 499 |  5.859565515s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:42 ishan-Legion ollama[70048]: INFO [main] build info | build=1 commit="1e6f655" tid="140608785547264" timestamp=1738854642
Feb 06 20:40:42 ishan-Legion ollama[70048]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="140608785547264" timestamp=1738854642 total_threads=16
Feb 06 20:40:42 ishan-Legion ollama[70048]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="41551" tid="140608785547264" timestamp=1738854642
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:42 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.189+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.322138377 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:42 ishan-Legion ollama[2704]: time=2025-02-06T20:40:42.439+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.572020437 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.267+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.128609051 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.417+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.418+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 37293"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.419+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.419+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.419+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.419+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:40:47 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:40:47 | 499 |  5.505557366s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:40:47 ishan-Legion ollama[70203]: INFO [main] build info | build=1 commit="1e6f655" tid="129571640455168" timestamp=1738854647
Feb 06 20:40:47 ishan-Legion ollama[70203]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129571640455168" timestamp=1738854647 total_threads=16
Feb 06 20:40:47 ishan-Legion ollama[70203]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="37293" tid="129571640455168" timestamp=1738854647
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.518+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.379204807 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.767+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.628394267 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.957+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.8 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.958+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama3383558217/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 36761"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.958+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.958+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:40:47 ishan-Legion ollama[2704]: time=2025-02-06T20:40:47.958+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:40:47 ishan-Legion ollama[70243]: INFO [main] build info | build=1 commit="1e6f655" tid="129178686967808" timestamp=1738854647
Feb 06 20:40:47 ishan-Legion ollama[70243]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129178686967808" timestamp=1738854647 total_threads=16
Feb 06 20:40:47 ishan-Legion ollama[70243]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="36761" tid="129178686967808" timestamp=1738854647
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:40:47 ishan-Legion ollama[2704]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:40:48 ishan-Legion ollama[2704]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer          = 27
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_head           = 16
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_rot            = 64
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_swa            = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert         = 64
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: causal attn      = 1
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: pooling type     = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: rope type        = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: model type       = 16B
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: max token length = 256
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:40:48 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:40:48 ishan-Legion ollama[2704]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:40:48 ishan-Legion ollama[2704]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:40:48 ishan-Legion ollama[2704]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:40:48 ishan-Legion ollama[2704]: time=2025-02-06T20:40:48.210+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:40:48 ishan-Legion ollama[2704]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:40:49 ishan-Legion ollama[2704]: time=2025-02-06T20:40:49.665+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:40:51 ishan-Legion ollama[2704]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:40:51 ishan-Legion ollama[2704]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:40:51 ishan-Legion ollama[2704]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:40:51 ishan-Legion ollama[2704]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:40:51 ishan-Legion ollama[2704]: time=2025-02-06T20:40:51.269+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:40:55 ishan-Legion ollama[2704]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:40:55 ishan-Legion ollama[2704]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:41:00 ishan-Legion ollama[2704]: llama_new_context_with_model: graph splits = 336
Feb 06 20:41:31 ishan-Legion ollama[70243]: INFO [main] model loaded | tid="129178686967808" timestamp=1738854691
Feb 06 20:41:31 ishan-Legion ollama[2704]: time=2025-02-06T20:41:31.665+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:42:00 ishan-Legion ollama[2704]: time=2025-02-06T20:42:00.932+05:30 level=INFO source=server.go:632 msg="llama runner started in 72.97 seconds"
Feb 06 20:42:15 ishan-Legion ollama[2704]: [GIN] 2025/02/06 - 20:42:15 | 200 |         1m29s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:44:54 ishan-Legion systemd[1]: Stopping Ollama Service...
Feb 06 20:44:57 ishan-Legion systemd[1]: ollama.service: Deactivated successfully.
Feb 06 20:44:57 ishan-Legion systemd[1]: Stopped Ollama Service.
Feb 06 20:44:57 ishan-Legion systemd[1]: ollama.service: Consumed 8min 59.046s CPU time.
Feb 06 20:47:47 ishan-Legion systemd[1]: Started Ollama Service.
Feb 06 20:47:47 ishan-Legion ollama[76682]: 2025/02/06 20:47:47 routes.go:1125: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/usr/share/ollama/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]"
Feb 06 20:47:47 ishan-Legion ollama[76682]: time=2025-02-06T20:47:47.156+05:30 level=INFO source=images.go:782 msg="total blobs: 6"
Feb 06 20:47:47 ishan-Legion ollama[76682]: time=2025-02-06T20:47:47.157+05:30 level=INFO source=images.go:790 msg="total unused blobs removed: 0"
Feb 06 20:47:47 ishan-Legion ollama[76682]: time=2025-02-06T20:47:47.157+05:30 level=INFO source=routes.go:1172 msg="Listening on [::]:11434 (version 0.3.6)"
Feb 06 20:47:47 ishan-Legion ollama[76682]: time=2025-02-06T20:47:47.157+05:30 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama4110796402/runners
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.642+05:30 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]"
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.642+05:30 level=INFO source=gpu.go:204 msg="looking for compatible GPUs"
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.728+05:30 level=WARN source=amd_linux.go:59 msg="ollama recommends running the https://www.amd.com/en/support/linux-drivers" error="amdgpu version file missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or directory"
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.728+05:30 level=INFO source=amd_linux.go:274 msg="unsupported Radeon iGPU detected skipping" id=0 total="512.0 MiB"
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.728+05:30 level=INFO source=amd_linux.go:360 msg="no compatible amdgpu devices detected"
Feb 06 20:47:49 ishan-Legion ollama[76682]: time=2025-02-06T20:47:49.728+05:30 level=INFO source=types.go:105 msg="inference compute" id=GPU-1740bb34-db30-45e9-1f5f-7b2ca53380a5 library=cuda compute=8.9 driver=12.2 name="NVIDIA GeForce RTX 4060 Laptop GPU" total="7.8 GiB" available="6.7 GiB"
Feb 06 20:49:09 ishan-Legion ollama[76682]: time=2025-02-06T20:49:09.839+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:49:09 ishan-Legion ollama[76682]: time=2025-02-06T20:49:09.840+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama4110796402/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 45777"
Feb 06 20:49:09 ishan-Legion ollama[76682]: time=2025-02-06T20:49:09.841+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:49:09 ishan-Legion ollama[76682]: time=2025-02-06T20:49:09.841+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:49:09 ishan-Legion ollama[76682]: time=2025-02-06T20:49:09.841+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:49:09 ishan-Legion ollama[77990]: INFO [main] build info | build=1 commit="1e6f655" tid="127944012439552" timestamp=1738855149
Feb 06 20:49:09 ishan-Legion ollama[77990]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="127944012439552" timestamp=1738855149 total_threads=16
Feb 06 20:49:09 ishan-Legion ollama[77990]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="45777" tid="127944012439552" timestamp=1738855149
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:49:09 ishan-Legion ollama[76682]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:49:09 ishan-Legion ollama[76682]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_layer          = 27
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_head           = 16
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_rot            = 64
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_swa            = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert         = 64
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: causal attn      = 1
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: pooling type     = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: rope type        = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: model type       = 16B
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: max token length = 256
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:49:10 ishan-Legion ollama[76682]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:49:10 ishan-Legion ollama[76682]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:49:10 ishan-Legion ollama[76682]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:49:10 ishan-Legion ollama[76682]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:49:10 ishan-Legion ollama[76682]: time=2025-02-06T20:49:10.092+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:49:10 ishan-Legion ollama[76682]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:49:10 ishan-Legion ollama[76682]: time=2025-02-06T20:49:10.343+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:49:10 ishan-Legion ollama[76682]: time=2025-02-06T20:49:10.343+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:49:10 ishan-Legion ollama[76682]: [GIN] 2025/02/06 - 20:49:10 | 499 |   591.94106ms |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.428+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.084731911 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.560+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.561+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama4110796402/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 33287"
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.562+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.562+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.562+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.562+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:49:15 ishan-Legion ollama[76682]: [GIN] 2025/02/06 - 20:49:15 | 499 |  5.227838988s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:49:15 ishan-Legion ollama[78308]: INFO [main] build info | build=1 commit="1e6f655" tid="124283906764800" timestamp=1738855155
Feb 06 20:49:15 ishan-Legion ollama[78308]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="124283906764800" timestamp=1738855155 total_threads=16
Feb 06 20:49:15 ishan-Legion ollama[78308]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="33287" tid="124283906764800" timestamp=1738855155
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:49:15 ishan-Legion ollama[76682]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.678+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.335107082 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:15 ishan-Legion ollama[76682]: time=2025-02-06T20:49:15.929+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.585517209 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.629+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.067712942 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.761+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.7 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.762+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama4110796402/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 34857"
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.762+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.762+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.762+05:30 level=WARN source=server.go:600 msg="client connection closed before server finished loading, aborting load"
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.762+05:30 level=ERROR source=sched.go:451 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
Feb 06 20:49:20 ishan-Legion ollama[76682]: [GIN] 2025/02/06 - 20:49:20 | 499 |   9.85423825s |       127.0.0.1 | POST     "/api/generate"
Feb 06 20:49:20 ishan-Legion ollama[78527]: INFO [main] build info | build=1 commit="1e6f655" tid="139526971748352" timestamp=1738855160
Feb 06 20:49:20 ishan-Legion ollama[78527]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="139526971748352" timestamp=1738855160 total_threads=16
Feb 06 20:49:20 ishan-Legion ollama[78527]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="34857" tid="139526971748352" timestamp=1738855160
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:49:20 ishan-Legion ollama[76682]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:49:20 ishan-Legion ollama[76682]: time=2025-02-06T20:49:20.879+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.317459221 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:21 ishan-Legion ollama[76682]: time=2025-02-06T20:49:21.130+05:30 level=WARN source=sched.go:642 msg="gpu VRAM usage didn't recover within timeout" seconds=5.56818581 model=/usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.212+05:30 level=INFO source=memory.go:309 msg="offload to cuda" layers.requested=-1 layers.model=28 layers.offload=6 layers.split="" memory.available="[6.6 GiB]" memory.required.full="21.3 GiB" memory.required.partial="6.6 GiB" memory.required.kv="10.4 GiB" memory.required.allocations="[6.6 GiB]" memory.weights.total="18.4 GiB" memory.weights.repeating="18.3 GiB" memory.weights.nonrepeating="164.1 MiB" memory.graph.full="1.3 GiB" memory.graph.partial="1.8 GiB"
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.213+05:30 level=INFO source=server.go:393 msg="starting llama server" cmd="/tmp/ollama4110796402/runners/cuda_v11/ollama_llama_server --model /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 --ctx-size 40480 --batch-size 512 --embedding --log-disable --n-gpu-layers 6 --no-mmap --parallel 1 --port 43777"
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.214+05:30 level=INFO source=sched.go:445 msg="loaded runners" count=1
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.214+05:30 level=INFO source=server.go:593 msg="waiting for llama runner to start responding"
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.214+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server error"
Feb 06 20:49:23 ishan-Legion ollama[78586]: INFO [main] build info | build=1 commit="1e6f655" tid="129726792011776" timestamp=1738855163
Feb 06 20:49:23 ishan-Legion ollama[78586]: INFO [main] system info | n_threads=8 n_threads_batch=-1 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="129726792011776" timestamp=1738855163 total_threads=16
Feb 06 20:49:23 ishan-Legion ollama[78586]: INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="15" port="43777" tid="129726792011776" timestamp=1738855163
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: loaded meta data with 38 key-value pairs and 377 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-5ff0abeeac1d2dbdd5455c0b49ba3b29a9ce3c1fb181b2eef2e948689d55d046 (version GGUF V3 (latest))
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   0:                       general.architecture str              = deepseek2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   1:                               general.name str              = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   2:                      deepseek2.block_count u32              = 27
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   3:                   deepseek2.context_length u32              = 163840
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   4:                 deepseek2.embedding_length u32              = 2048
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   5:              deepseek2.feed_forward_length u32              = 10944
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   6:             deepseek2.attention.head_count u32              = 16
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   7:          deepseek2.attention.head_count_kv u32              = 16
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   8:                   deepseek2.rope.freq_base f32              = 10000.000000
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv   9: deepseek2.attention.layer_norm_rms_epsilon f32              = 0.000001
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  10:                deepseek2.expert_used_count u32              = 6
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  11:                          general.file_type u32              = 2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  12:        deepseek2.leading_dense_block_count u32              = 1
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  13:                       deepseek2.vocab_size u32              = 102400
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  14:           deepseek2.attention.kv_lora_rank u32              = 512
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  15:             deepseek2.attention.key_length u32              = 192
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  16:           deepseek2.attention.value_length u32              = 128
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  17:       deepseek2.expert_feed_forward_length u32              = 1408
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  18:                     deepseek2.expert_count u32              = 64
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  19:              deepseek2.expert_shared_count u32              = 2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  20:             deepseek2.expert_weights_scale f32              = 1.000000
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  21:             deepseek2.rope.dimension_count u32              = 64
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  22:                deepseek2.rope.scaling.type str              = yarn
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  23:              deepseek2.rope.scaling.factor f32              = 40.000000
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  24: deepseek2.rope.scaling.original_context_length u32              = 4096
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  25: deepseek2.rope.scaling.yarn_log_multiplier f32              = 0.070700
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = deepseek-llm
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,102400]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,99757]   = [" ", " t", " a", "i n", "h e...
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 100000
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 100001
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 100001
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - kv  37:               general.quantization_version u32              = 2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - type  f32:  108 tensors
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - type q4_0:  268 tensors
Feb 06 20:49:23 ishan-Legion ollama[76682]: llama_model_loader: - type q6_K:    1 tensors
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_vocab: special tokens cache size = 2400
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_vocab: token to piece cache size = 0.6661 MB
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: format           = GGUF V3 (latest)
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: arch             = deepseek2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: vocab type       = BPE
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_vocab          = 102400
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_merges         = 99757
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: vocab_only       = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_ctx_train      = 163840
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd           = 2048
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_layer          = 27
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_head           = 16
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_head_kv        = 16
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_rot            = 64
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_swa            = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_head_k    = 192
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_head_v    = 128
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_gqa            = 1
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_k_gqa     = 3072
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_embd_v_gqa     = 2048
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: f_norm_eps       = 0.0e+00
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: f_logit_scale    = 0.0e+00
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_ff             = 10944
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert         = 64
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert_used    = 6
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: causal attn      = 1
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: pooling type     = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: rope type        = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: rope scaling     = yarn
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: freq_base_train  = 10000.0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: freq_scale_train = 0.025
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_ctx_orig_yarn  = 4096
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: rope_finetuned   = unknown
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_conv       = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_inner      = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_d_state      = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: ssm_dt_rank      = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: model type       = 16B
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: model ftype      = Q4_0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: model params     = 15.71 B
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: model size       = 8.29 GiB (4.53 BPW)
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: general.name     = DeepSeek-Coder-V2-Lite-Instruct
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: BOS token        = 100000 '<beginofsentence>'
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: EOS token        = 100001 '<endofsentence>'
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: PAD token        = 100001 '<endofsentence>'
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: LF token         = 126 ''
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: max token length = 256
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_layer_dense_lead   = 1
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_lora_q             = 0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_lora_kv            = 512
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_ff_exp             = 1408
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: n_expert_shared      = 2
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: expert_weights_scale = 1.0
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_print_meta: rope_yarn_log_mul    = 0.0707
Feb 06 20:49:23 ishan-Legion ollama[76682]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
Feb 06 20:49:23 ishan-Legion ollama[76682]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
Feb 06 20:49:23 ishan-Legion ollama[76682]: ggml_cuda_init: found 1 CUDA devices:
Feb 06 20:49:23 ishan-Legion ollama[76682]:   Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes
Feb 06 20:49:23 ishan-Legion ollama[76682]: time=2025-02-06T20:49:23.465+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:49:23 ishan-Legion ollama[76682]: llm_load_tensors: ggml ctx size =    0.32 MiB
Feb 06 20:49:24 ishan-Legion ollama[76682]: time=2025-02-06T20:49:24.920+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
Feb 06 20:49:26 ishan-Legion ollama[76682]: time=2025-02-06T20:49:26.180+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server loading model"
Feb 06 20:49:26 ishan-Legion ollama[76682]: llm_load_tensors: offloading 6 repeating layers to GPU
Feb 06 20:49:26 ishan-Legion ollama[76682]: llm_load_tensors: offloaded 6/28 layers to GPU
Feb 06 20:49:26 ishan-Legion ollama[76682]: llm_load_tensors:  CUDA_Host buffer size =  6603.67 MiB
Feb 06 20:49:26 ishan-Legion ollama[76682]: llm_load_tensors:      CUDA0 buffer size =  1885.09 MiB
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: n_ctx      = 40480
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: n_batch    = 512
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: n_ubatch   = 512
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: flash_attn = 0
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: freq_base  = 10000.0
Feb 06 20:49:31 ishan-Legion ollama[76682]: llama_new_context_with_model: freq_scale = 0.025
Feb 06 20:49:31 ishan-Legion ollama[76682]: ggml_cuda_host_malloc: failed to allocate 8301.56 MiB of pinned memory: out of memory
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_kv_cache_init:        CPU KV buffer size =  8301.56 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_kv_cache_init:      CUDA0 KV buffer size =  2371.88 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model: KV self size  = 10673.44 MiB, K (f16): 6404.06 MiB, V (f16): 4269.38 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model:  CUDA_Host  output buffer size =     0.40 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model:      CUDA0 compute buffer size =  1841.81 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model:  CUDA_Host compute buffer size =    89.07 MiB
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model: graph nodes  = 1924
Feb 06 20:49:37 ishan-Legion ollama[76682]: llama_new_context_with_model: graph splits = 336
Feb 06 20:50:08 ishan-Legion ollama[78586]: INFO [main] model loaded | tid="129726792011776" timestamp=1738855208
Feb 06 20:50:08 ishan-Legion ollama[76682]: time=2025-02-06T20:50:08.971+05:30 level=INFO source=server.go:627 msg="waiting for server to become available" status="llm server not responding"
